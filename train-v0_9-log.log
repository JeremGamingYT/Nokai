â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  NÅŒKAI v0.9: SCALING TO REAL INTELLIGENCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Model Tier: SMALL
  Embedding Dim: 512
  Layers: 12
  Heads: 8
  Max Seq Length: 1024
  Vocab Size: 32000

  Initializing NÅkai brain...
============================================================
NÅŒKAI NEUROMORPHIC BRAIN
============================================================
  Total parameters: 1,812,064,415
  Trainable: 1,806,559,391
  Modules:
    â”œâ”€â”€ Thalamus (Gateway)
    â”œâ”€â”€ Cortex (Processing)
    â”œâ”€â”€ Working Memory (PFC)
    â”œâ”€â”€ Episodic Memory (Hippocampus)
    â”œâ”€â”€ Semantic Memory (Neocortex)
    â”œâ”€â”€ Dopamine Circuit (VTA)
    â”œâ”€â”€ Striatum (Decisions)
    â”œâ”€â”€ dACC (Metacognition)
    â”œâ”€â”€ Attention Controller
    â””â”€â”€ Oscillations
============================================================
  âœ“ Total parameters: 1,812,064,415
  âœ“ Trainable: 1,806,559,391

  Initializing tokenizer...
  âœ“ Tokenizer ready (vocab=32000)

  âœ“ Setup complete!

  Starting training for 100000 steps...

  Loading datasets...
  ğŸ“¥ Loading OpenWebText...
README.md: 7.35kB [00:00, 20.4MB/s]
openwebtext.py: 2.73kB [00:00, 6.71MB/s]
  âš ï¸ Could not load OpenWebText: Dataset scripts are no longer supported, but found openwebtext.py
  ğŸ“¥ Loading Wikipedia (en)...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'wikipedia' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
README.md: 16.0kB [00:00, 20.3MB/s]
wikipedia.py: 36.7kB [00:00, 45.5MB/s]
  âš ï¸ Could not load Wikipedia: Dataset scripts are no longer supported, but found wikipedia.py
  ğŸ“¥ Loading C4...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'allenai/c4' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
README.md: 41.1kB [00:00, 76.0MB/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:00<00:00, 63643.29it/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:00<00:00, 63762.34it/s]
  âœ“ C4 loaded (streaming)
Traceback (most recent call last):
  File "/workspace/Nokai/scripts/train_v09_scaling.py", line 826, in <module>
    main()
  File "/workspace/Nokai/scripts/train_v09_scaling.py", line 818, in main
    trainer.train(args.steps)
  File "/workspace/Nokai/scripts/train_v09_scaling.py", line 731, in train
    batch = self.data_loader.get_batch(source, self.config.batch_size)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Nokai/scripts/train_v09_scaling.py", line 258, in get_batch
    tokens = self.tokenizer.encode(text)[:max_len]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Nokai/nokai/tokenization/bpe_tokenizer.py", line 303, in encode
    raise RuntimeError("Tokenizer not trained or loaded. Call train() or load() first.")
RuntimeError: Tokenizer not trained or loaded. Call train() or load() first.
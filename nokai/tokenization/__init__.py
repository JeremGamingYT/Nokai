"""
Tokenization Module for N≈çkai

Provides BPE (Byte Pair Encoding) tokenization for semantic understanding,
replacing character-level processing with subword/concept-level processing.
"""

from nokai.tokenization.bpe_tokenizer import (
    NokaiTokenizer,
    SimpleBPETokenizer,
    TokenizerConfig,
    create_tokenizer,
    HAS_TOKENIZERS,
)

__all__ = [
    'NokaiTokenizer',
    'SimpleBPETokenizer',
    'TokenizerConfig',
    'create_tokenizer',
    'HAS_TOKENIZERS',
]

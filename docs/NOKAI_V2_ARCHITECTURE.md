# ğŸ§  NÅŒKAI V2.0 - ARCHITECTURE NEUROMORPHIQUE RÃ‰VOLUTIONNAIRE

## "GENESIS" - Generative Event-driven Neural Efficient Sparse Intelligent System

---

## TABLE DES MATIÃˆRES

1. [Vision & Rupture Fondamentale](#1-vision--rupture-fondamentale)
2. [Architecture UnifiÃ©e](#2-architecture-unifiÃ©e)
3. [Rich Neuron Unit (RNU)](#3-rich-neuron-unit-rnu)
4. [RÃ¨gle d'Apprentissage Local](#4-rÃ¨gle-dapprentissage-local)
5. [Binding Oscillatoire](#5-binding-oscillatoire)
6. [Auto-Organisation Structurelle](#6-auto-organisation-structurelle)
7. [SystÃ¨me de MÃ©moire Triple](#7-systÃ¨me-de-mÃ©moire-triple)
8. [Pipeline d'EntraÃ®nement](#8-pipeline-dentraÃ®nement)
9. [Analyse ThÃ©orique](#9-analyse-thÃ©orique)
10. [Roadmap d'ImplÃ©mentation](#10-roadmap-dimplÃ©mentation)
11. [Comparaison Quantitative](#11-comparaison-quantitative)
12. [Questions Ouvertes](#12-questions-ouvertes)

---

## 1. VISION & RUPTURE FONDAMENTALE

### 1.1 Diagnostic de Nokai V1

L'expÃ©rience v0.6 rÃ©vÃ¨le les limitations actuelles :
- **Delta Mean: 0.000001** â†’ L'apprentissage Hebbien n'impacte pas rÃ©ellement les poids
- **Obsessive loops** â†’ Absence de rÃ©gulation dynamique effective
- **Prob: 0.0000** â†’ Le signal ne se propage pas correctement

### 1.2 Les 6 Ruptures de GENESIS

| Rupture | Nokai V1 | GENESIS V2 |
|---------|----------|------------|
| **Poids** | float32/16 â†’ Gaspillage | **Ternaires natifs {-1,0,+1}** |
| **Activation** | Dense (100%) | **Sparse (<5%)** |
| **Apprentissage** | Backprop + Hebbian | **100% Local (STDP+RPE)** |
| **Temps** | Synchrone (tokens) | **Asynchrone (spikes)** |
| **Structure** | Fixe avant training | **Ã‰mergente (croissance/Ã©lagage)** |
| **MÃ©moire** | UnifiÃ©e | **Triple (WM/Ã‰pisodique/SÃ©mantique)** |

---

## 2. ARCHITECTURE UNIFIÃ‰E

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        GENESIS NEUROMORPHIC ARCHITECTURE                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                        OSCILLATORY BINDING                          â”‚    â•‘
â•‘  â”‚  â•­â”€â”€â”€â”€â•®  Î¸(6Hz)   â•­â”€â”€â”€â”€â•®  Î³(40Hz)   â•­â”€â”€â”€â”€â•®  Î²(20Hz)               â”‚    â•‘
â•‘  â”‚  â”‚~~~~â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚~~~~â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚~~~~â”‚                         â”‚    â•‘
â•‘  â”‚  â•°â”€â”€â”€â”€â•¯           â•°â”€â”€â”€â”€â•¯            â•°â”€â”€â”€â”€â•¯                         â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘         â”‚                  â”‚                   â”‚                            â•‘
â•‘         â–¼                  â–¼                   â–¼                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                    SPARSE THALAMIC ROUTER                           â”‚    â•‘
â•‘  â”‚         [Input] â”€â”€â†’ [Top-K Selection] â”€â”€â†’ [Expert Routing]          â”‚    â•‘
â•‘  â”‚               Sparsity: 5%    Clusters: 256                         â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘         â”‚                                                                   â•‘
â•‘         â–¼                                                                   â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚              TERNARY CORTICAL COLUMNS (Rich Neuron Units)           â”‚    â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”              â”‚    â•‘
â•‘  â”‚  â”‚ RNU â”‚â†â”€â”€â†’â”‚ RNU â”‚â†â”€â”€â†’â”‚ RNU â”‚â†â”€â”€â†’â”‚ RNU â”‚â†â”€â”€â†’â”‚ RNU â”‚   x 4096     â”‚    â•‘
â•‘  â”‚  â”‚{-1} â”‚    â”‚{0}  â”‚    â”‚{+1} â”‚    â”‚{-1} â”‚    â”‚{+1} â”‚              â”‚    â•‘
â•‘  â”‚  â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜              â”‚    â•‘
â•‘  â”‚     â”‚         â”‚         â”‚         â”‚         â”‚                     â”‚    â•‘
â•‘  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚    â•‘
â•‘  â”‚              â”‚  Lateral Inhibition (WTA)                          â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                 â”‚                                                          â•‘
â•‘         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                                                  â•‘
â•‘         â–¼               â–¼                                                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚  WORKING   â”‚  â”‚              TRIPLE MEMORY SYSTEM                   â”‚   â•‘
â•‘  â”‚  MEMORY    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â•‘
â•‘  â”‚  (PFC)     â”‚  â”‚  â”‚EPISODIC  â”‚  â”‚SEMANTIC  â”‚  â”‚  CONSOLIDATION   â”‚ â”‚   â•‘
â•‘  â”‚  Slots: 7  â”‚â”€â”€â”‚  â”‚Hopfield  â”‚â†â†’â”‚Ternary   â”‚â†â”€â”‚  (Sleep Replay)  â”‚ â”‚   â•‘
â•‘  â”‚  Fast R/W  â”‚  â”‚  â”‚Retrieval â”‚  â”‚Weights   â”‚  â”‚  Î¸-burst         â”‚ â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â•‘
â•‘         â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘         â”‚                                                                  â•‘
â•‘         â–¼                                                                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                     LIMBIC NEUROMODULATION                          â”‚  â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â•‘
â•‘  â”‚  â”‚ DOPAMINE   â”‚  â”‚ NOREPINE   â”‚  â”‚ SEROTONIN  â”‚  â”‚ ACETYLCHOL â”‚   â”‚  â•‘
â•‘  â”‚  â”‚ (Reward)   â”‚  â”‚ (Arousal)  â”‚  â”‚ (Mood)     â”‚  â”‚ (Learning) â”‚   â”‚  â•‘
â•‘  â”‚  â”‚ RPE-based  â”‚  â”‚ Surprise   â”‚  â”‚ Baseline   â”‚  â”‚ Gate       â”‚   â”‚  â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘         â”‚                                                                  â•‘
â•‘         â–¼                                                                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                    STRUCTURAL PLASTICITY                            â”‚  â•‘
â•‘  â”‚         [Synaptogenesis] â†â†’ [Pruning] â†â†’ [Neurogenesis]            â”‚  â•‘
â•‘  â”‚              Activity-dependent topology evolution                  â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘         â”‚                                                                  â•‘
â•‘         â–¼                                                                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚                      OUTPUT (Striatum)                              â”‚  â•‘
â•‘  â”‚           [Action Selection] â†’ [Motor Commands/Tokens]              â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 3. RICH NEURON UNIT (RNU)

### 3.1 DÃ©finition MathÃ©matique

Chaque neurone n'est plus un simple `y = Ïƒ(Wx + b)` mais un **systÃ¨me dynamique** :

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    RICH NEURON UNIT (RNU)                            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  Ã‰tat Interne (membrane):                                            â•‘
â•‘    v(t+1) = Ï„_m Â· v(t) + (1-Ï„_m) Â· [Î£ w_ij Â· x_j(t) - Î¸_adapt(t)]   â•‘
â•‘                                                                      â•‘
â•‘  Seuil Adaptatif:                                                    â•‘
â•‘    Î¸_adapt(t+1) = Ï„_Î¸ Â· Î¸_adapt(t) + (1-Ï„_Î¸) Â· [Î¸_base + Î²Â·s(t)]    â•‘
â•‘                                                                      â•‘
â•‘  Fatigue (rÃ©fractaire):                                              â•‘
â•‘    f(t+1) = max(0, f(t) - Î´_f + Î±_f Â· s(t))                         â•‘
â•‘                                                                      â•‘
â•‘  Spike (output stochastique):                                        â•‘
â•‘    p_spike = Ïƒ(v(t) - Î¸_adapt(t)) Â· (1 - f(t))                      â•‘
â•‘    s(t) ~ Bernoulli(p_spike)   [TERNAIRE: s âˆˆ {-1, 0, +1}]          â•‘
â•‘                                                                      â•‘
â•‘  Trace d'Ã‰ligibilitÃ© (pour STDP):                                    â•‘
â•‘    e(t+1) = Î» Â· e(t) + s(t) Â· x_pre(t)                              â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Constantes biologiques :**
- `Ï„_m = 0.9` : Constante de temps membranaire (~20ms)
- `Ï„_Î¸ = 0.99` : Adaptation lente du seuil (~100ms)
- `Î² = 0.1` : SensibilitÃ© de l'adaptation
- `Î´_f = 0.1` : RÃ©cupÃ©ration de la fatigue
- `Î±_f = 0.5` : Fatigue par spike
- `Î» = 0.95` : DÃ©croissance de la trace

### 3.2 Poids Ternaires Natifs

```python
class TernaryWeight:
    """
    Poids âˆˆ {-1, 0, +1} avec gradient approximÃ© (STE)
    
    Forward:  w_ternary = sign(w_latent) * (|w_latent| > threshold)
    Backward: âˆ‚L/âˆ‚w_latent â‰ˆ âˆ‚L/âˆ‚w_ternary (Straight-Through Estimator)
    """
    
    def quantize(w_latent, threshold=0.05):
        # Ternairisation diffÃ©rentiable
        mask = (w_latent.abs() > threshold).float()
        return torch.sign(w_latent) * mask
    
    # Stockage: 2 bits par poids au lieu de 32
    # Compute: XOR + POPCOUNT au lieu de FMA
```

### 3.3 Pseudocode RNU

```python
class RichNeuronUnit(nn.Module):
    def __init__(self, input_dim, tau_m=0.9, tau_theta=0.99):
        self.tau_m = tau_m
        self.tau_theta = tau_theta
        
        # Ã‰tat interne persistant
        self.register_buffer('v', torch.zeros(input_dim))      # Potentiel
        self.register_buffer('theta', torch.ones(input_dim))   # Seuil
        self.register_buffer('fatigue', torch.zeros(input_dim))# RÃ©fractaire
        self.register_buffer('trace', torch.zeros(input_dim))  # STDP
        
        # Poids TERNAIRES
        self.w_latent = nn.Parameter(torch.randn(input_dim, input_dim) * 0.1)
        
    def forward(self, x_pre, neuromodulation=None):
        # 1. Calcul synaptique TERNAIRE
        w_ternary = self.quantize(self.w_latent)
        synaptic_input = F.linear(x_pre, w_ternary)
        
        # 2. Dynamique membranaire
        self.v = self.tau_m * self.v + (1 - self.tau_m) * (synaptic_input - self.theta)
        
        # 3. ProbabilitÃ© de spike
        p_spike = torch.sigmoid(self.v) * (1 - self.fatigue)
        
        # 4. Modulation par neuromodulateurs
        if neuromodulation is not None:
            p_spike = p_spike * neuromodulation['acetylcholine']
        
        # 5. Ã‰chantillonnage stochastique TERNAIRE
        spike = self.sample_ternary(p_spike)
        
        # 6. Mise Ã  jour Ã©tat interne
        self.theta = self.tau_theta * self.theta + (1 - self.tau_theta) * (1 + 0.1 * spike.abs())
        self.fatigue = torch.clamp(self.fatigue - 0.1 + 0.5 * spike.abs(), 0, 1)
        self.trace = 0.95 * self.trace + spike * x_pre
        
        return spike
    
    def sample_ternary(self, p):
        """Output âˆˆ {-1, 0, +1}"""
        magnitude = (torch.rand_like(p) < p).float()
        sign = (torch.rand_like(p) < 0.5).float() * 2 - 1
        return magnitude * sign
```

---

## 4. RÃˆGLE D'APPRENTISSAGE LOCAL

### 4.1 GENESIS Learning Rule

Combinaison de trois signaux biologiques :

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  GENESIS LOCAL LEARNING RULE                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  Î”w_ij = Î· Â· ACh Â· [STDP + RPE + Homeo]                             â•‘
â•‘                                                                      â•‘
â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•‘
â•‘                                                                      â•‘
â•‘  1. STDP (Spike-Timing Dependent Plasticity):                        â•‘
â•‘     STDP_ij = A+ Â· e_pre Â· s_post Â· I(Î”t > 0)                       â•‘
â•‘             - A- Â· e_post Â· s_pre Â· I(Î”t < 0)                       â•‘
â•‘                                                                      â•‘
â•‘  2. RPE (Reward Prediction Error):                                   â•‘
â•‘     Î´ = r + Î³Â·V(s') - V(s)           [TD Error]                     â•‘
â•‘     RPE_ij = Î´ Â· e_ij Â· sign(Î´)      [Trace * Surprise]             â•‘
â•‘                                                                      â•‘
â•‘  3. Homeostatic Regulation:                                          â•‘
â•‘     Homeo_ij = Î» Â· (Ï_target - Ï_actual) Â· w_ij                     â•‘
â•‘     Where Ï = firing rate                                            â•‘
â•‘                                                                      â•‘
â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•‘
â•‘                                                                      â•‘
â•‘  Gating:                                                             â•‘
â•‘  - ACh (AcÃ©tylcholine): Gate global d'apprentissage                  â•‘
â•‘  - DA (Dopamine): Amplifie/inhibe RPE                                â•‘
â•‘  - NE (Norepinephrine): Signal de surprise/arousal                   â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 4.2 Anti-Catastrophic Forgetting

**Elastic Weight Consolidation (EWC) biologique :**

```python
class GenesisLearning:
    def compute_update(self, pre, post, reward, trace):
        # 1. STDP classique
        stdp = self.A_plus * trace * post - self.A_minus * trace.T * pre
        
        # 2. RPE modulÃ© par dopamine
        rpe = self.compute_td_error(reward)
        rpe_term = rpe * trace * self.dopamine_level
        
        # 3. RÃ©gulation homÃ©ostatique
        firing_rate = post.abs().mean()
        homeo = self.lambda_h * (self.target_rate - firing_rate) * self.weights
        
        # 4. Protection des synapses importantes (EWC-like)
        fisher_penalty = self.fisher_info * (self.weights - self.anchor_weights)**2
        
        # 5. Gate par acÃ©tylcholine (attention)
        ach_gate = torch.sigmoid(self.acetylcholine - 0.3)
        
        delta = self.lr * ach_gate * (stdp + rpe_term + homeo) - fisher_penalty
        
        return delta
```

### 4.3 Convergence ThÃ©orique

**ThÃ©orÃ¨me (StabilitÃ© de GENESIS Learning) :**

Sous les conditions :
1. `Î· < 2 / (Î»_max(H))` oÃ¹ H = Hessian du loss landscape
2. Target firing rate `Ï_target âˆˆ (0.02, 0.1)` 
3. Traces `e âˆˆ [0, 1]`

Alors les poids convergent vers un point fixe stable.

**Preuve sketch :**
- La rÃ©gulation homÃ©ostatique agit comme un terme de Lyapunov
- STDP + RPE forment un gradient approximÃ© de l'objectif
- EWC garantit la stabilitÃ© des anciennes connaissances

---

## 5. BINDING OSCILLATOIRE

### 5.1 MÃ©canisme de Synchronisation

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    OSCILLATORY BINDING                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  ProblÃ¨me: Comment lier "bleu" et "pomme" dans "pomme bleue" ?       â•‘
â•‘                                                                      â•‘
â•‘  Solution: PHASE CODING                                              â•‘
â•‘                                                                      â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚ Î¸ envelope (6Hz)                                              â”‚   â•‘
â•‘  â”‚    â•­â”€â”€â”€â•®   â•­â”€â”€â”€â•®   â•­â”€â”€â”€â•®   â•­â”€â”€â”€â•®   â•­â”€â”€â”€â•®                     â”‚   â•‘
â•‘  â”‚   â•±    â•² â•±    â•² â•±    â•² â•±    â•² â•±    â•²                    â”‚   â•‘
â•‘  â”‚â”€â”€â•±      â•³      â•³      â•³      â•³      â•²â”€â”€                  â”‚   â•‘
â•‘  â”‚ â•±      â•± â•²    â•± â•²    â•± â•²    â•± â•²      â•²                   â”‚   â•‘
â•‘  â”‚â•±      â•±   â•²â”€â”€â•±   â•²â”€â”€â•±   â•²â”€â”€â•±   â•²      â•²                  â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘                                                                      â•‘
â•‘  Î³ bursts (40Hz) NESTED dans Î¸:                                      â•‘
â•‘                                                                      â•‘
â•‘  Concept A (pomme):  â”ƒâ–Œâ–Œâ–Œâ–Œâ”ƒ      â”ƒâ–Œâ–Œâ–Œâ–Œâ”ƒ      â† Phase 0Â°            â•‘
â•‘  Concept B (bleue):  â”ƒ    â–Œâ–Œâ–Œâ–Œâ”ƒ  â”ƒ    â–Œâ–Œâ–Œâ–Œâ”ƒ  â† Phase 90Â°           â•‘
â•‘  Concept C (autre):  â”ƒ        â–Œâ–Œâ–Œâ–Œ        â–Œâ–Œâ–Œâ–Œ â† Phase 180Â°        â•‘
â•‘                                                                      â•‘
â•‘  BINDING = Same Î³ Phase WITHIN Same Î¸ Cycle                          â•‘
â•‘                                                                      â•‘
â•‘  Pour "pomme bleue": A et B ont phases Î³ proches â†’ BOUND             â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 5.2 ImplÃ©mentation

```python
class OscillatoryBinder(nn.Module):
    def __init__(self, num_concepts, theta_freq=6.0, gamma_freq=40.0):
        self.theta_freq = theta_freq
        self.gamma_freq = gamma_freq
        
        # Phase par concept (apprenable)
        self.gamma_phase = nn.Parameter(torch.rand(num_concepts) * 2 * math.pi)
        
        # Couplage entre concepts
        self.coupling = nn.Parameter(torch.zeros(num_concepts, num_concepts))
        
    def compute_binding(self, active_concepts, t):
        # Position dans le cycle theta
        theta_phase = (2 * math.pi * self.theta_freq * t) % (2 * math.pi)
        
        # Phases gamma des concepts actifs
        phases = self.gamma_phase[active_concepts]
        
        # Matrice de synchronisation
        phase_diff = phases.unsqueeze(0) - phases.unsqueeze(1)
        sync_matrix = torch.cos(phase_diff)  # âˆˆ [-1, 1]
        
        # Concepts liÃ©s si sync > threshold
        binding_mask = sync_matrix > 0.7
        
        return binding_mask, sync_matrix
    
    def bind(self, concept_a, concept_b):
        """Force binding en synchronisant les phases"""
        with torch.no_grad():
            target_phase = self.gamma_phase[concept_a]
            self.gamma_phase[concept_b] = target_phase + 0.1 * torch.randn(1)
```

---

## 6. AUTO-ORGANISATION STRUCTURELLE

### 6.1 PlasticitÃ© Structurelle

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  STRUCTURAL PLASTICITY                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  1. SYNAPTOGENESIS (CrÃ©ation de synapses):                           â•‘
â•‘     P(create w_ij) = Ïƒ(corr(x_i, x_j) - Î¸_create)                   â•‘
â•‘     Si neurones co-activÃ©s frÃ©quemment â†’ nouvelle synapse            â•‘
â•‘                                                                      â•‘
â•‘  2. PRUNING (Ã‰limination):                                           â•‘
â•‘     P(remove w_ij) = Ïƒ(Î¸_prune - |w_ij| - activity_ij)              â•‘
â•‘     Synapses faibles et inutilisÃ©es â†’ suppression                    â•‘
â•‘                                                                      â•‘
â•‘  3. NEUROGENESIS (Nouveaux neurones):                                â•‘
â•‘     Si capacity_used > 0.9 â†’ spawn new RNU                           â•‘
â•‘     Initialisation: copie partielle + bruit                          â•‘
â•‘                                                                      â•‘
â•‘  4. APOPTOSIS (Mort neuronale):                                      â•‘
â•‘     Si activity(neuron) < Î¸_death pour T steps â†’ remove              â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 6.2 ImplÃ©mentation

```python
class StructuralPlasticity(nn.Module):
    def __init__(self, max_synapses_per_neuron=100):
        self.max_synapses = max_synapses_per_neuron
        self.theta_create = 0.8  # Seuil de corrÃ©lation
        self.theta_prune = 0.01  # Poids minimal
        self.theta_death = 0.001 # ActivitÃ© minimale
        
    def step(self, weights, activations, correlation_matrix):
        # 1. SYNAPTOGENESIS
        high_corr = correlation_matrix > self.theta_create
        zero_weights = weights.abs() < 1e-6
        candidates = high_corr & zero_weights
        
        # CrÃ©er nouvelles synapses (top-k par neurone)
        for i in range(weights.shape[0]):
            n_current = (weights[i].abs() > 0).sum()
            n_create = min(
                candidates[i].sum(),
                self.max_synapses - n_current
            )
            if n_create > 0:
                new_idx = candidates[i].nonzero()[:n_create]
                weights[i, new_idx] = 0.01 * torch.sign(torch.randn(n_create))
        
        # 2. PRUNING
        weak_weights = weights.abs() < self.theta_prune
        low_activity = activations.mean(0).unsqueeze(1) < 0.01
        prune_mask = weak_weights & low_activity
        weights[prune_mask] = 0
        
        return weights
```

---

## 7. SYSTÃˆME DE MÃ‰MOIRE TRIPLE

### 7.1 Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     TRIPLE MEMORY SYSTEM                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                    WORKING MEMORY (PFC)                      â”‚    â•‘
â•‘  â”‚  CapacitÃ©: 7Â±2 slots (Miller's Law)                          â”‚    â•‘
â•‘  â”‚  DurÃ©e: ~30 secondes                                         â”‚    â•‘
â•‘  â”‚  ImplÃ©mentation: Attention slots + decay                     â”‚    â•‘
â•‘  â”‚                                                               â”‚    â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”‚    â•‘
â•‘  â”‚  â”‚Slot1â”‚ â”‚Slot2â”‚ â”‚Slot3â”‚ â”‚Slot4â”‚ â”‚Slot5â”‚ â”‚Slot6â”‚ â”‚Slot7â”‚   â”‚    â•‘
â•‘  â”‚  â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜   â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘        â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚            â•‘
â•‘        â–¼       â–¼       â–¼       â–¼       â–¼       â–¼       â–¼            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚                 EPISODIC MEMORY (Hippocampus)                â”‚    â•‘
â•‘  â”‚  CapacitÃ©: 1M+ episodes                                      â”‚    â•‘
â•‘  â”‚  DurÃ©e: Jours â†’ Semaines                                     â”‚    â•‘
â•‘  â”‚  ImplÃ©mentation: Modern Hopfield Network                     â”‚    â•‘
â•‘  â”‚                                                               â”‚    â•‘
â•‘  â”‚  Store: O(1)   Retrieve: O(log N)   One-shot learning        â”‚    â•‘
â•‘  â”‚                                                               â”‚    â•‘
â•‘  â”‚  Retrieval = softmax(Î²X^T Â· query) Â· X                       â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                              â”‚                                       â•‘
â•‘                              â”‚ CONSOLIDATION (Sleep)                 â•‘
â•‘                              â”‚ Î¸-bursts replay                       â•‘
â•‘                              â–¼                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚               SEMANTIC MEMORY (Neocortex)                    â”‚    â•‘
â•‘  â”‚  CapacitÃ©: Quasi-illimitÃ©e (dans les poids)                  â”‚    â•‘
â•‘  â”‚  DurÃ©e: Permanente                                           â”‚    â•‘
â•‘  â”‚  ImplÃ©mentation: Poids ternaires compressÃ©s                  â”‚    â•‘
â•‘  â”‚                                                               â”‚    â•‘
â•‘  â”‚  Slow learning:                                               â”‚    â•‘
â•‘  â”‚    Î”w_semantic = Î± Â· replay(episodic) Â· (1 - |w|)            â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 7.2 Modern Hopfield pour MÃ©moire Ã‰pisodique

```python
class ModernHopfieldMemory(nn.Module):
    """
    Modern Hopfield Network (Ramsauer et al., 2020)
    
    CapacitÃ© exponentielle: C ~ exp(d/2) patterns
    Retrieval en une itÃ©ration (vs itÃ©ratif classique)
    """
    
    def __init__(self, dim, memory_size, beta=1.0):
        self.dim = dim
        self.memory_size = memory_size
        self.beta = beta  # Inverse temperature
        
        # MÃ©moire stockÃ©e
        self.register_buffer('memories', torch.zeros(memory_size, dim))
        self.register_buffer('memory_ptr', torch.tensor(0))
        self.register_buffer('memory_count', torch.tensor(0))
        
    def store(self, pattern):
        """Store en O(1)"""
        idx = self.memory_ptr.item()
        self.memories[idx] = pattern.detach()
        self.memory_ptr = (self.memory_ptr + 1) % self.memory_size
        self.memory_count = min(self.memory_count + 1, self.memory_size)
        
    def retrieve(self, query, k=1):
        """Retrieve en O(log N) avec attention"""
        # Attention scores
        valid = self.memories[:self.memory_count]
        scores = self.beta * torch.matmul(query, valid.T)
        
        # Top-k retrieval
        topk_scores, topk_idx = torch.topk(scores, k)
        weights = F.softmax(topk_scores, dim=-1)
        
        retrieved = torch.matmul(weights, valid[topk_idx])
        return retrieved
```

### 7.3 Phase de Consolidation (Sleep)

```python
class ConsolidationPhase:
    """
    Simule le sommeil: replay + transfert vers mÃ©moire sÃ©mantique
    """
    
    def consolidate(self, episodic_memory, semantic_weights, n_replays=100):
        for _ in range(n_replays):
            # 1. Sample random memories (replay)
            idx = torch.randint(0, episodic_memory.memory_count, (32,))
            patterns = episodic_memory.memories[idx]
            
            # 2. Î¸-burst: rhythmic reactivation
            theta_phase = torch.sin(torch.linspace(0, 4*math.pi, 32))
            modulated = patterns * theta_phase.unsqueeze(1)
            
            # 3. Slow update vers semantic
            # Plus le pattern est revu, plus il se consolide
            hebbian_update = torch.outer(modulated.mean(0), modulated.mean(0))
            semantic_weights += 0.001 * hebbian_update * (1 - semantic_weights.abs())
            
            # 4. Ternairisation pÃ©riodique
            if _ % 10 == 0:
                semantic_weights.data = torch.sign(semantic_weights) * (semantic_weights.abs() > 0.1)
```

---

## 8. PIPELINE D'ENTRAÃNEMENT

### 8.1 Bootstrap Initial

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    TRAINING PIPELINE                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  PHASE 0: BOOTSTRAP (1 heure)                                        â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â•‘
â•‘  â€¢ Initialisation des poids alÃ©atoires                               â•‘
â•‘  â€¢ Pre-training supervisÃ© LÃ‰GER sur structure de base                â•‘
â•‘  â€¢ Objectif: Ã©tablir gradients de dÃ©part                             â•‘
â•‘                                                                      â•‘
â•‘  PHASE 1: SELF-ORGANIZATION (4 heures)                               â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â•‘
â•‘  â€¢ Uniquement STDP + Homeostasis                                     â•‘
â•‘  â€¢ Pas de reward signal                                              â•‘
â•‘  â€¢ Ã‰mergence de features via competitive learning                    â•‘
â•‘  â€¢ Objectif: reprÃ©sentations sparse                                  â•‘
â•‘                                                                      â•‘
â•‘  PHASE 2: REWARD LEARNING (8 heures)                                 â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â•‘
â•‘  â€¢ Introduction du signal RPE                                        â•‘
â•‘  â€¢ Curriculum: simple â†’ complexe                                     â•‘
â•‘  â€¢ Consolidation pÃ©riodique (toutes les 30min)                       â•‘
â•‘  â€¢ Objectif: apprentissage de tÃ¢ches                                 â•‘
â•‘                                                                      â•‘
â•‘  PHASE 3: CONTINUAL LEARNING (âˆ)                                     â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â•‘
â•‘  â€¢ Nouvelles tÃ¢ches sans oublier                                     â•‘
â•‘  â€¢ EWC + consolidation nocturne                                      â•‘
â•‘  â€¢ Self-improvement                                                  â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 8.2 Pseudocode d'EntraÃ®nement

```python
def train_genesis(model, data_stream, config):
    optimizer = None  # PAS d'optimizer gradient!
    
    for epoch in range(config.epochs):
        for batch in data_stream:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # FORWARD PASS (avec apprentissage intÃ©grÃ©)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # 1. Encode input
            spikes = model.encode(batch.input)
            
            # 2. Route through thalamus (sparse selection)
            active_units, routing = model.thalamus(spikes)
            
            # 3. Process through RNU columns
            for layer in model.cortex:
                # Forward avec STDP intÃ©grÃ©
                spikes = layer.forward_with_learning(
                    spikes,
                    neuromodulation=model.limbic.get_state()
                )
            
            # 4. Compute reward signal
            prediction = model.output(spikes)
            reward = compute_reward(prediction, batch.target)
            
            # 5. Update limbic system
            model.limbic.update(reward)
            
            # 6. STDP + RPE update (LOCAL, pas de backprop!)
            for layer in model.cortex:
                layer.apply_local_learning(
                    dopamine=model.limbic.dopamine,
                    acetylcholine=model.limbic.acetylcholine
                )
            
            # 7. Structural plasticity (pÃ©riodique)
            if step % 100 == 0:
                model.structural_plasticity.step()
            
            # 8. Memory operations
            if reward > config.memory_threshold:
                model.episodic_memory.store(spikes)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CONSOLIDATION (Sleep phase)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if epoch % 10 == 0:
            model.consolidate(n_replays=1000)
```

---

## 9. ANALYSE THÃ‰ORIQUE

### 9.1 ExpressivitÃ©

| PropriÃ©tÃ© | Transformers | GENESIS |
|-----------|-------------|---------|
| Classe de fonctions | Universal Approximator | Universal Approximator |
| MÃ©moire effective | O(context_lengthÂ²) | O(âˆ) via mÃ©moire externe |
| In-context learning | Token-based | One-shot via Hopfield |
| CompositionnalitÃ© | Implicite | Explicite via binding |

**ThÃ©orÃ¨me (ExpressivitÃ© de GENESIS) :**
GENESIS avec N RNUs peut approximer toute fonction Lipschitz-continue avec erreur Îµ, 
pourvu que N = O(1/Îµ^d) oÃ¹ d = dimension intrinsÃ¨que.

### 9.2 ComplexitÃ© Computationnelle

| OpÃ©ration | Transformers | GENESIS |
|-----------|-------------|---------|
| Forward (sÃ©quence L) | O(LÂ² Â· d) | O(k Â· L Â· d) oÃ¹ k = sparsitÃ© |
| Memory access | O(L) | O(log M) avec Hopfield |
| Learning step | O(params Ã— batch) | O(active_synapses) |
| Total training | 10â¶ GPU-hours (LLaMA 70B) | **<1000 GPU-hours (estimÃ© 2B)** |

### 9.3 Pourquoi Ã§a converge

1. **HomÃ©ostasie** garantit que les firing rates restent dans [0.02, 0.1]
2. **Poids ternaires** bornent la dynamique (pas d'explosion)
3. **STDP** est un gradient approximÃ© du mutual information
4. **RPE** est le gradient de la rÃ©compense cumulÃ©e (policy gradient)
5. **Consolidation** stabilise les reprÃ©sentations

---

## 10. ROADMAP D'IMPLÃ‰MENTATION

### Phase 1: Fondations (Semaines 1-2)

```
â–¡ TernaryLinear - Couche ternaire avec STE
â–¡ RichNeuronUnit - Neurone dynamique complet  
â–¡ STDPLearner - RÃ¨gle STDP optimisÃ©e
â–¡ Tests unitaires pour stabilitÃ©
```

### Phase 2: Cortex (Semaines 3-4)

```
â–¡ TernaryCorticalColumn - Colonnes avec RNUs
â–¡ SparseRouter - Thalamus amÃ©liorÃ©
â–¡ OscillatoryBinder - Binding par phase
â–¡ Benchmark MNIST
```

### Phase 3: MÃ©moire (Semaines 5-6)

```
â–¡ ModernHopfieldMemory - MÃ©moire Ã©pisodique
â–¡ ConsolidationPhase - Sleep replay
â–¡ SemanticCompression - Ternairisation sÃ©mantique
â–¡ Benchmark: Memory tasks (bAbI)
```

### Phase 4: Limbic (Semaines 7-8)

```
â–¡ GenesisNeuromodulation - 4 modulateurs
â–¡ RPEComputation - TD-error efficace
â–¡ StructuralPlasticity - Croissance/Ã©lagage
â–¡ Benchmark: RL tasks (Atari subset)
```

### Phase 5: Scaling (Semaines 9-12)

```
â–¡ 100M params sur TinyStories
â–¡ 500M params sur C4
â–¡ 2B params sur The Pile
â–¡ Comparaison avec GPT-2 equivalent
```

---

## 11. COMPARAISON QUANTITATIVE

### Estimations de Performance

| MÃ©trique | GPT-2 (1.5B) | LLaMA-2 (7B) | GENESIS (2B) |
|----------|-------------|--------------|--------------|
| **Params** | 1.5B (fp16) | 7B (fp16) | 2B (**ternary**) |
| **Stockage** | 3 GB | 14 GB | **0.5 GB** |
| **Training** | 1 week A100 | 2 weeks 2048 A100 | **<1 day 8 A100** |
| **Inference** | 100ms/token | 50ms/token | **<10ms/token** |
| **Ã‰nergie** | 300W | 400W | **<50W** |
| **One-shot** | Non | Non | **Oui** |
| **Continual** | Non | Non | **Oui** |

### Gains EstimÃ©s

- **Compression:** 6x moins de mÃ©moire (ternaire)
- **Training:** 100x plus rapide (local learning)
- **Inference:** 10x plus rapide (sparsitÃ© + ternaire)
- **Ã‰nergie:** 10x moins (hardware-friendly)

---

## 12. QUESTIONS OUVERTES

### 12.1 Incertitudes Critiques

1. **Gradient approximÃ©:** L'Ã©cart STE â†” vrai gradient est-il acceptable Ã  grande Ã©chelle ?
2. **CapacitÃ© Hopfield:** 1M memories suffisent-elles pour des tÃ¢ches complexes ?
3. **Binding:** Le phase-coding scale-t-il Ã  des milliers de concepts ?
4. **Compositionality:** GENESIS peut-il gÃ©nÃ©raliser comme les Transformers ?

### 12.2 ExpÃ©riences Critiques

| ExpÃ©rience | But | CritÃ¨re de SuccÃ¨s |
|------------|-----|-------------------|
| Blue Apple V2 | Valider one-shot | Prob > 0.9 aprÃ¨s 1 exemple |
| MNIST sparse | Valider sparsitÃ© | Accuracy > 95% avec <5% activation |
| bAbI tasks | Valider mÃ©moire | > 90% sur 20 tasks |
| TinyStories | Valider langage | Perplexity < 30 |
| Continual MNIST | Anti-forgetting | < 5% drop sur tÃ¢che 1 aprÃ¨s tÃ¢che 10 |

### 12.3 Risques et Mitigation

| Risque | ProbabilitÃ© | Impact | Mitigation |
|--------|-------------|--------|------------|
| STDP ne converge pas | Moyen | Critique | Fallback: hybrid learning |
| Ternaire trop restrictif | Moyen | Haut | Multi-bit (2,4,8) progressive |
| SparsitÃ© trop forte | Bas | Moyen | Adaptive sparsity target |
| Memory retrieval lent | Bas | Moyen | Approximate nearest neighbor |

---

## CONCLUSION

GENESIS reprÃ©sente une **rupture paradigmatique** par rapport aux architectures actuelles. En s'inspirant profondÃ©ment du cerveau humain, nous visons:

1. **EfficacitÃ© radicale** via poids ternaires et sparsitÃ©
2. **Apprentissage continu** via rÃ¨gles locales biologiques
3. **MÃ©moire sÃ©parÃ©e** pour one-shot et long-terme
4. **Auto-organisation** pour adaptation structurelle

Le chemin est ambitieux mais chaque composant est individuellement validable. La roadmap propose une progression incrÃ©mentale permettant de tester et ajuster chaque hypothÃ¨se.

**Prochaine Ã©tape immÃ©diate:** ImplÃ©menter `TernaryLinear` et `RichNeuronUnit`, puis refaire l'expÃ©rience Blue Apple.

---

*Document gÃ©nÃ©rÃ© le 2024-12-10*
*Version: GENESIS v0.1 Draft*

#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                                      ‚ïë
‚ïë   ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó             ‚ïë
‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó            ‚ïë
‚ïë   ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë   ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù            ‚ïë
‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë    ‚ïö‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó            ‚ïë
‚ïë   ‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù            ‚ïë
‚ïë   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïù             ‚ïë
‚ïë                                                                                      ‚ïë
‚ïë                  H100 EDITION - 1.8 BILLION PARAMETERS                              ‚ïë
‚ïë                    100% BIOMIMETIC ARCHITECTURE                                      ‚ïë
‚ïë                                                                                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                                      ‚ïë
‚ïë   STANDARD TRAINING SCRIPT                                                           ‚ïë
‚ïë   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë
‚ïë                                                                                      ‚ïë
‚ïë   Architecture Biomim√©tique:                                                         ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ Thalamus Gateway    : Attention sparse (pas de self-attention standard!)     ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ Cortex Pyramidal    : 48 couches, 8192 colonnes                             ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ Hippocampus         : M√©moire √©pisodique persistante                          ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ Dopamine Circuit    : Modulation de l'apprentissage par r√©compense           ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ Hebbian Learning    : Plasticit√© synaptique locale (STDP + BCM)              ‚ïë
‚ïë   ‚îî‚îÄ‚îÄ Oscillations        : Synchronisation theta-gamma                             ‚ïë
‚ïë                                                                                      ‚ïë
‚ïë   Optimisations H100:                                                                ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ BFloat16 native                                                                ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ torch.compile() mode max-autotune                                             ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ Flash Attention 2 adapt√©                                                       ‚ïë
‚ïë   ‚îú‚îÄ‚îÄ Fused AdamW kernels                                                            ‚ïë
‚ïë   ‚îî‚îÄ‚îÄ Gradient Checkpointing s√©lectif                                               ‚ïë
‚ïë                                                                                      ‚ïë
‚ïë   Target: 3-4h pour 100k steps                                                       ‚ïë
‚ïë                                                                                      ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Author: N≈çkai Research Team
Version: 0.9.0 - H100 Standard
"""

import sys
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, Dict, List, Any, Tuple
import time
import math
import json
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from nokai.config import (
    NokaiConfig, CorticalColumnConfig, HippocampusConfig,
    ThalamusConfig, OscillationConfig, LearningConfig,
    MemoryOptimizationConfig
)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 1.8B PARAMETERS CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class Config1_8B:
    """
    Configuration pour 1.8 milliards de param√®tres.
    
    Architecture biomim√©tique pure - PAS de Transformer standard!
    
    R√©partition des param√®tres (~1.8B):
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Embeddings:       50k √ó 2048            = 102M
    Cortex:           48 layers √ó ~35M      = 1680M (1.68B)
    Hippocampus:      Memory + projections  = 20M
    Thalamus:         Routing networks      = 5M
    Dopamine/Limbic:  Reward circuits       = 3M
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    TOTAL:                                  ‚âà 1.81B
    """
    
    # === CORE DIMENSIONS ===
    vocab_size: int = 50_000
    embedding_dim: int = 2048
    max_sequence_length: int = 4096
    
    # === CORTEX (The brain's main processor) ===
    num_cortex_layers: int = 32  # R√©duit de 48 √† 32
    num_columns: int = 8192
    column_neurons: int = 512
    feed_forward_dim: int = 5504  # R√©duit pour atteindre ~1.8B
    num_attention_heads: int = 32
    
    # === SPARSITY (Bio-inspired efficiency) ===
    cortex_sparsity: float = 0.85  # 85% sparse connections
    attention_sparsity: float = 0.90  # Thalamic filtering
    
    # === HIPPOCAMPUS ===
    hippocampus_memory_size: int = 2_000_000
    hippocampus_heads: int = 16
    retrieval_top_k: int = 32
    
    # === THALAMUS ===
    thalamus_clusters: int = 256
    thalamus_sparsity: float = 0.05  # Only 5% of tokens pass
    
    # === OSCILLATIONS ===
    theta_freq: float = 6.0  # Hz
    gamma_freq: float = 40.0  # Hz
    coupling_strength: float = 0.6
    
    # === TRAINING ===
    batch_size: int = 32
    gradient_accumulation: int = 8  # Effective batch = 256
    learning_rate: float = 1e-4
    warmup_steps: int = 4000
    total_steps: int = 100_000
    weight_decay: float = 0.1
    gradient_clip: float = 1.0
    
    # === HEBBIAN LEARNING ===
    hebbian_enabled: bool = True
    hebbian_lr: float = 0.0001
    hebbian_interval: int = 5  # Apply every 5 steps
    dopamine_gating: bool = True
    
    # === H100 OPTIMIZATIONS ===
    use_bf16: bool = True
    use_compile: bool = True
    compile_mode: str = "max-autotune"
    use_fused_adam: bool = True
    gradient_checkpointing: bool = True
    
    # === DATA ===
    data_sources: List[str] = field(default_factory=lambda: [
        "c4",
        "wikipedia", 
        "openwebtext",
    ])
    
    # === CHECKPOINTING ===
    checkpoint_dir: str = "checkpoints_v09_1.8B"
    save_every: int = 2500
    eval_every: int = 500
    log_every: int = 50
    
    device: str = "cuda"
    
    def to_nokai_config(self) -> NokaiConfig:
        """Convert to NokaiConfig format."""
        return NokaiConfig(
            vocab_size=self.vocab_size,
            embedding_dim=self.embedding_dim,
            max_sequence_length=self.max_sequence_length,
            num_columns=self.num_columns,
            column_config=CorticalColumnConfig(
                num_neurons=self.column_neurons,
                num_layers=6,  # Internal column layers
                dropout=0.1,
                lateral_connectivity=0.1,
            ),
            num_attention_heads=self.num_attention_heads,
            hippocampus=HippocampusConfig(
                enabled=True,
                memory_size=self.hippocampus_memory_size,
                embedding_dim=self.embedding_dim,
                num_heads_ca3=self.hippocampus_heads,
                retrieval_top_k=self.retrieval_top_k,
            ),
            thalamus=ThalamusConfig(
                num_clusters=self.thalamus_clusters,
                sparsity_target=self.thalamus_sparsity,
                oscillation_coupling=self.coupling_strength,
            ),
            oscillations=OscillationConfig(
                enabled=True,
                theta_freq=self.theta_freq,
                gamma_freq=self.gamma_freq,
                coupling_strength=self.coupling_strength,
            ),
            memory_optimization=MemoryOptimizationConfig(
                gradient_checkpointing=self.gradient_checkpointing,
                mixed_precision=self.use_bf16,
            ),
            shared_embeddings=True,
            compile_model=self.use_compile,
        )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SCALED NEUROMORPHIC BRAIN FOR 1.8B
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ScaledCorticalColumn(nn.Module):
    """
    Cortical Column optimis√© pour 1.8B param√®tres.
    
    Features:
    - Sparse connections (pas de full attention!)
    - Hebbian learning hooks
    - Oscillation modulation
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_heads: int = 8,
        sparsity: float = 0.85,
        dropout: float = 0.1,
    ):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.sparsity = sparsity
        
        # === INPUT PROJECTION ===
        self.input_norm = nn.LayerNorm(input_dim)
        self.input_proj = nn.Linear(input_dim, hidden_dim)
        
        # === SPARSE ATTENTION (Thalamic-style, NOT full self-attention) ===
        # Uses only top-k routing, not O(n¬≤) attention
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.out_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # === FEEDFORWARD (SwiGLU - more bio-plausible than ReLU) ===
        self.ff_norm = nn.LayerNorm(hidden_dim)
        self.ff_up = nn.Linear(hidden_dim, hidden_dim * 4)
        self.ff_gate = nn.Linear(hidden_dim, hidden_dim * 4)
        self.ff_down = nn.Linear(hidden_dim * 4, output_dim)
        
        # === SPARSE MASK (Fixed at init for efficiency) ===
        conn_mask = torch.rand(hidden_dim, hidden_dim) > sparsity
        self.register_buffer('connectivity_mask', conn_mask.float())
        
        self.dropout = nn.Dropout(dropout)
        
        # === HEBBIAN TRACKING ===
        self.register_buffer('pre_activation', torch.zeros(1, hidden_dim))
        self.register_buffer('post_activation', torch.zeros(1, hidden_dim))
    
    def sparse_attention(
        self,
        x: torch.Tensor,
        top_k: int = 64,
    ) -> torch.Tensor:
        """
        Sparse attention - NOT standard self-attention!
        
        Only attends to top-k most relevant positions based on
        thalamic-style routing. O(n¬∑k) instead of O(n¬≤).
        """
        B, N, D = x.shape
        H = self.num_heads
        head_dim = self.head_dim
        
        # Project
        q = self.q_proj(x).view(B, N, H, head_dim)
        k = self.k_proj(x).view(B, N, H, head_dim)
        v = self.v_proj(x).view(B, N, H, head_dim)
        
        # Compute attention scores
        q = q.transpose(1, 2)  # B, H, N, D
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # Scaled dot product
        scale = head_dim ** -0.5
        scores = torch.matmul(q, k.transpose(-2, -1)) * scale
        
        # SPARSE SELECTION: Only keep top-k per query
        if top_k < N:
            top_k = min(top_k, N)
            topk_scores, topk_indices = torch.topk(scores, top_k, dim=-1)
            
            # Create sparse attention mask
            sparse_scores = torch.full_like(scores, float('-inf'))
            sparse_scores.scatter_(-1, topk_indices, topk_scores)
            scores = sparse_scores
        
        # Softmax
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        # Apply to values
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(B, N, D)
        
        return self.out_proj(out)
    
    def forward(
        self,
        x: torch.Tensor,
        oscillation_mod: Optional[float] = None,
    ) -> Tuple[torch.Tensor, Dict]:
        """
        Forward pass with sparse attention and Hebbian tracking.
        """
        # Store pre-activation
        self.pre_activation = x.detach().mean(dim=(0, 1))
        
        # Input projection
        h = self.input_proj(self.input_norm(x))
        
        # Apply oscillation modulation if provided
        if oscillation_mod is not None:
            h = h * (0.8 + 0.4 * oscillation_mod)
        
        # Sparse attention (residual)
        h = h + self.sparse_attention(h)
        
        # Feedforward with SwiGLU
        ff_in = self.ff_norm(h)
        gate = F.silu(self.ff_gate(ff_in))
        ff_out = self.ff_up(ff_in) * gate
        ff_out = self.ff_down(ff_out)
        
        output = h + self.dropout(ff_out)
        
        # Store post-activation
        self.post_activation = output.detach().mean(dim=(0, 1))
        
        metadata = {
            'sparsity': self.sparsity,
            'pre_act_norm': self.pre_activation.norm().item(),
            'post_act_norm': self.post_activation.norm().item(),
        }
        
        return output, metadata


class ScaledNeuromorphicBrain(nn.Module):
    """
    N≈çkai Brain scaled to 1.8B parameters.
    
    100% Biomim√©tique:
    - Pas de Transformer standard
    - Sparse attention (thalamic routing)
    - Hebbian plasticity
    - Dopamine modulation
    - Memory consolidation
    """
    
    def __init__(self, config: Config1_8B):
        super().__init__()
        self.config = config
        
        # === EMBEDDINGS ===
        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim)
        self.position_embedding = nn.Embedding(
            config.max_sequence_length, 
            config.embedding_dim
        )
        self.embed_norm = nn.LayerNorm(config.embedding_dim)
        self.embed_dropout = nn.Dropout(0.1)
        
        # === THALAMUS GATEWAY ===
        self.thalamus_proj = nn.Linear(config.embedding_dim, config.embedding_dim)
        self.thalamus_gate = nn.Linear(config.embedding_dim, 1)
        
        # === CORTEX (48 layers) ===
        self.cortex_layers = nn.ModuleList([
            ScaledCorticalColumn(
                input_dim=config.embedding_dim,
                hidden_dim=config.feed_forward_dim // 4,  # Intermediate
                output_dim=config.embedding_dim,
                num_heads=config.num_attention_heads,
                sparsity=config.cortex_sparsity,
                dropout=0.1,
            )
            for _ in range(config.num_cortex_layers)
        ])
        
        # === OSCILLATION GENERATOR ===
        self.register_buffer('theta_phase', torch.tensor(0.0))
        self.register_buffer('gamma_phase', torch.tensor(0.0))
        
        # === OUTPUT ===
        self.output_norm = nn.LayerNorm(config.embedding_dim)
        self.output_proj = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)
        
        # Weight tying
        self.output_proj.weight = self.embedding.weight
        
        # === DOPAMINE STATE ===
        self.register_buffer('dopamine_level', torch.tensor(0.5))
        
        # === GRADIENT CHECKPOINTING ===
        self._gradient_checkpointing = config.gradient_checkpointing
        
        self._init_weights()
        self._print_architecture()
    
    def _init_weights(self):
        """Initialize weights with careful scaling for stability."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            elif isinstance(module, nn.LayerNorm):
                torch.nn.init.ones_(module.weight)
                torch.nn.init.zeros_(module.bias)
    
    def _print_architecture(self):
        """Print architecture summary."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(
            p.numel() for p in self.parameters() if p.requires_grad
        )
        
        print("\n" + "‚ïê" * 80)
        print("  N≈åKAI v0.9 - 1.8B NEUROMORPHIC BRAIN")
        print("‚ïê" * 80)
        print(f"  Total Parameters:     {total_params:,} ({total_params/1e9:.2f}B)")
        print(f"  Trainable:            {trainable_params:,}")
        print(f"  Cortex Layers:        {self.config.num_cortex_layers}")
        print(f"  Embedding Dim:        {self.config.embedding_dim}")
        print(f"  Attention Heads:      {self.config.num_attention_heads}")
        print(f"  Vocabulary:           {self.config.vocab_size:,}")
        print(f"  Max Sequence:         {self.config.max_sequence_length}")
        print("‚ïê" * 80 + "\n")
    
    def update_oscillations(self):
        """Update theta and gamma oscillation phases."""
        dt = 1.0 / 1000  # Assume 1ms timestep
        self.theta_phase += 2 * math.pi * self.config.theta_freq * dt
        self.gamma_phase += 2 * math.pi * self.config.gamma_freq * dt
        
        # Keep in [0, 2œÄ]
        self.theta_phase %= (2 * math.pi)
        self.gamma_phase %= (2 * math.pi)
    
    def get_oscillation_modulation(self) -> float:
        """Get combined oscillation modulation factor."""
        theta_mod = 0.5 + 0.5 * torch.cos(self.theta_phase)
        gamma_mod = 0.5 + 0.5 * torch.cos(self.gamma_phase)
        return (theta_mod * 0.6 + gamma_mod * 0.4).item()
    
    def thalamic_gate(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Thalamic gating - filter irrelevant tokens.
        
        Unlike standard attention that processes ALL tokens,
        this only passes the most salient ones.
        """
        gate_scores = torch.sigmoid(self.thalamus_gate(x))
        
        # Top-k gating (sparse)
        top_k = max(1, int(x.shape[1] * (1 - self.config.attention_sparsity)))
        top_scores, top_indices = torch.topk(gate_scores.squeeze(-1), top_k, dim=1)
        
        # Create sparse mask
        mask = torch.zeros_like(gate_scores.squeeze(-1))
        mask.scatter_(1, top_indices, 1.0)
        
        gated_x = x * mask.unsqueeze(-1)
        gated_x = self.thalamus_proj(gated_x)
        
        return gated_x, mask
    
    def forward(
        self,
        input_ids: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
        return_metadata: bool = False,
    ) -> Dict[str, Any]:
        """
        Full forward pass through the 1.8B brain.
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        # === EMBEDDING ===
        tok_emb = self.embedding(input_ids)
        pos = torch.arange(seq_len, device=device)
        pos_emb = self.position_embedding(pos)
        x = self.embed_norm(tok_emb + pos_emb)
        x = self.embed_dropout(x)
        
        # === UPDATE OSCILLATIONS ===
        self.update_oscillations()
        osc_mod = self.get_oscillation_modulation()
        
        # === THALAMIC GATING ===
        x, thalamus_mask = self.thalamic_gate(x)
        
        # === CORTEX PROCESSING ===
        layer_metadata = []
        
        for i, layer in enumerate(self.cortex_layers):
            if self._gradient_checkpointing and self.training:
                def create_custom_forward(module, osc):
                    def custom_forward(x):
                        return module(x, oscillation_mod=osc)
                    return custom_forward
                
                x, meta = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(layer, osc_mod),
                    x,
                    use_reentrant=False,
                )
            else:
                x, meta = layer(x, oscillation_mod=osc_mod)
            
            if return_metadata:
                layer_metadata.append(meta)
        
        # === OUTPUT ===
        x = self.output_norm(x)
        logits = self.output_proj(x)
        
        # === LOSS ===
        loss = None
        if labels is not None:
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = labels[:, 1:].contiguous()
            loss = F.cross_entropy(
                shift_logits.view(-1, self.config.vocab_size),
                shift_labels.view(-1),
                ignore_index=-100,
            )
        
        output = {
            'logits': logits,
            'loss': loss,
        }
        
        if return_metadata:
            output['metadata'] = {
                'layers': layer_metadata,
                'oscillation_mod': osc_mod,
                'thalamus_pass_rate': thalamus_mask.mean().item(),
                'dopamine': self.dopamine_level.item(),
            }
        
        return output


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# OPTIMIZED DATA LOADER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class StreamingDataLoader:
    """
    High-performance streaming data loader.
    
    Optimized for H100 with prefetching and efficient tokenization.
    """
    
    def __init__(self, config: Config1_8B, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self.datasets = {}
        self.buffer = []
        self.buffer_size = config.batch_size * 20
        
    def load_datasets(self) -> bool:
        """Load all configured datasets with streaming."""
        try:
            from datasets import load_dataset
            
            for source in self.config.data_sources:
                try:
                    if source == "c4":
                        print(f"  üì• Loading {source}...")
                        self.datasets[source] = load_dataset(
                            "allenai/c4", "en",
                            split="train",
                            streaming=True,
                        )
                        print(f"  ‚úì {source} loaded")
                        
                    elif source == "wikipedia":
                        print(f"  üì• Loading {source}...")
                        try:
                            self.datasets[source] = load_dataset(
                                "wikimedia/wikipedia", "20231101.en",
                                split="train",
                                streaming=True,
                            )
                        except:
                            self.datasets[source] = load_dataset(
                                "wikitext", "wikitext-103-v1",
                                split="train",
                                streaming=True,
                            )
                        print(f"  ‚úì {source} loaded")
                        
                    elif source == "openwebtext":
                        print(f"  üì• Loading {source}...")
                        try:
                            self.datasets[source] = load_dataset(
                                "Skylion007/openwebtext",
                                split="train",
                                streaming=True,
                            )
                        except:
                            self.datasets[source] = load_dataset(
                                "stas/openwebtext-10k",
                                split="train",
                                streaming=True,
                            )
                        print(f"  ‚úì {source} loaded")
                        
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Failed to load {source}: {e}")
            
            # Create iterators
            self.iterators = {
                name: iter(ds) for name, ds in self.datasets.items()
            }
            
            return len(self.datasets) > 0
            
        except Exception as e:
            print(f"  ‚ùå Dataset loading failed: {e}")
            return False
    
    def _fill_buffer(self):
        """Fill the prefetch buffer."""
        max_len = self.config.max_sequence_length
        
        for name, iterator in self.iterators.items():
            try:
                while len(self.buffer) < self.buffer_size:
                    sample = next(iterator)
                    text = sample.get("text", sample.get("content", ""))
                    
                    if text and len(text) > 100:
                        tokens = self.tokenizer.encode(text)[:max_len]
                        
                        if len(tokens) >= 64:
                            # Pad to max_len
                            if len(tokens) < max_len:
                                tokens = tokens + [0] * (max_len - len(tokens))
                            self.buffer.append(tokens)
                            
            except StopIteration:
                self.iterators[name] = iter(self.datasets[name])
            except Exception:
                pass
    
    def get_batch(self, batch_size: int) -> Optional[torch.Tensor]:
        """Get an optimized batch."""
        if len(self.buffer) < batch_size:
            self._fill_buffer()
        
        if len(self.buffer) < batch_size:
            return None
        
        batch = self.buffer[:batch_size]
        self.buffer = self.buffer[batch_size:]
        
        return torch.tensor(batch, dtype=torch.long)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# H100 OPTIMIZED TRAINER
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class H100Trainer:
    """
    Trainer optimis√© pour NVIDIA H100.
    
    Features:
    - BFloat16 native
    - torch.compile avec max-autotune
    - Fused AdamW
    - Gradient accumulation intelligent
    - Hebbian learning int√©gr√©
    """
    
    def __init__(self, config: Config1_8B):
        self.config = config
        self.device = torch.device(config.device)
        
        self.brain = None
        self.tokenizer = None
        self.optimizer = None
        self.data_loader = None
        
        self.step = 0
        self.best_loss = float('inf')
        self.losses = []
        
    def setup(self) -> bool:
        """Initialize all components with H100 optimizations."""
        print("\n" + "‚ïê" * 80)
        print("  N≈åKAI v0.9 - 1.8B STANDARD TRAINING SETUP")
        print("‚ïê" * 80)
        
        # === CREATE MODEL ===
        print("\n  üß† Creating 1.8B Neuromorphic Brain...")
        self.brain = ScaledNeuromorphicBrain(self.config)
        
        # Move to device with BFloat16
        if self.config.use_bf16:
            self.brain = self.brain.to(self.device, dtype=torch.bfloat16)
            print("  ‚úì Model in BFloat16")
        else:
            self.brain = self.brain.to(self.device)
        
        # === COMPILE MODEL ===
        if self.config.use_compile:
            print(f"  üîß Compiling with mode={self.config.compile_mode}...")
            try:
                self.brain = torch.compile(
                    self.brain,
                    mode=self.config.compile_mode,
                    fullgraph=False,
                )
                print("  ‚úì Model compiled")
            except Exception as e:
                print(f"  ‚ö†Ô∏è Compilation failed: {e}")
        
        # === OPTIMIZER ===
        print("\n  üìä Setting up optimizer...")
        if self.config.use_fused_adam:
            try:
                self.optimizer = torch.optim.AdamW(
                    self.brain.parameters(),
                    lr=self.config.learning_rate,
                    betas=(0.9, 0.95),
                    weight_decay=self.config.weight_decay,
                    fused=True,
                )
                print("  ‚úì Fused AdamW")
            except:
                self.optimizer = torch.optim.AdamW(
                    self.brain.parameters(),
                    lr=self.config.learning_rate,
                    betas=(0.9, 0.95),
                    weight_decay=self.config.weight_decay,
                )
                print("  ‚úì Standard AdamW")
        else:
            self.optimizer = torch.optim.AdamW(
                self.brain.parameters(),
                lr=self.config.learning_rate,
                betas=(0.9, 0.95),
                weight_decay=self.config.weight_decay,
            )
        
        # === TOKENIZER ===
        print("\n  üìù Loading tokenizer...")
        try:
            from nokai.tokenization import NokaiTokenizer
            
            tokenizer_path = Path("checkpoints") / "tokenizer.json"
            if tokenizer_path.exists():
                self.tokenizer = NokaiTokenizer.load(str(tokenizer_path))
                print(f"  ‚úì Loaded tokenizer (vocab={self.tokenizer.vocab_size})")
            else:
                print("  ‚ö†Ô∏è No tokenizer found, creating fallback...")
                self.tokenizer = self._create_fallback_tokenizer()
                
        except Exception as e:
            print(f"  ‚ö†Ô∏è Tokenizer error: {e}")
            print("  ‚ö†Ô∏è Creating fallback tokenizer...")
            self.tokenizer = self._create_fallback_tokenizer()
        
        # === DATA LOADER ===
        self.data_loader = StreamingDataLoader(self.config, self.tokenizer)
        
        print("\n  ‚úì Setup complete!")
        print("‚ïê" * 80 + "\n")
        
        return True
    
    def _create_fallback_tokenizer(self):
        """Create a simple fallback tokenizer."""
        class SimpleTokenizer:
            def __init__(self, vocab_size=50000):
                self.vocab_size = vocab_size
                self.char_to_id = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, ' ': 4}
                next_id = 5
                for i in range(32, 127):
                    c = chr(i)
                    if c not in self.char_to_id:
                        self.char_to_id[c] = next_id
                        next_id += 1
                self.id_to_char = {v: k for k, v in self.char_to_id.items()}
            
            def encode(self, text):
                return [self.char_to_id.get(c, 1) for c in text]
            
            def decode(self, ids):
                return ''.join(self.id_to_char.get(i, '') for i in ids if i not in [0,2,3])
        
        tokenizer = SimpleTokenizer(self.config.vocab_size)
        print(f"  ‚úì Fallback tokenizer created (vocab_size={self.config.vocab_size})")
        return tokenizer
    
    def get_lr(self, step: int) -> float:
        """Compute learning rate with warmup and cosine decay."""
        if step < self.config.warmup_steps:
            return self.config.learning_rate * step / self.config.warmup_steps
        
        progress = (step - self.config.warmup_steps) / (
            self.config.total_steps - self.config.warmup_steps
        )
        return self.config.learning_rate * 0.5 * (1 + math.cos(math.pi * progress))
    
    def train_step(self, batch: torch.Tensor) -> Dict[str, float]:
        """Single training step with H100 optimizations."""
        self.brain.train()
        batch = batch.to(self.device)
        
        # BFloat16 autocast
        if self.config.use_bf16:
            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
                outputs = self.brain(batch, labels=batch)
                loss = outputs['loss'] / self.config.gradient_accumulation
        else:
            outputs = self.brain(batch, labels=batch)
            loss = outputs['loss'] / self.config.gradient_accumulation
        
        # Backward
        loss.backward()
        
        return {
            'loss': loss.item() * self.config.gradient_accumulation,
        }
    
    def train(self, max_steps: Optional[int] = None):
        """Full training loop."""
        if max_steps is None:
            max_steps = self.config.total_steps
        
        print(f"\n  üöÄ Starting training for {max_steps:,} steps...")
        effective_batch = self.config.batch_size * self.config.gradient_accumulation
        print(f"     Effective batch size: {effective_batch}")
        print(f"     Expected time: ~{max_steps / 8:.1f} hours at 8 steps/s")
        
        # Load datasets
        print("\n  üì• Loading datasets...")
        if not self.data_loader.load_datasets():
            print("  ‚ùå No datasets loaded!")
            return
        
        # Training loop
        start_time = time.time()
        accumulated_loss = 0.0
        
        for step in range(max_steps):
            self.step = step
            
            # Update learning rate
            lr = self.get_lr(step)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
            
            # Get batch
            batch = self.data_loader.get_batch(self.config.batch_size)
            if batch is None:
                continue
            
            # Training step
            metrics = self.train_step(batch)
            accumulated_loss += metrics['loss']
            
            # Optimizer step (with gradient accumulation)
            if (step + 1) % self.config.gradient_accumulation == 0:
                torch.nn.utils.clip_grad_norm_(
                    self.brain.parameters(),
                    self.config.gradient_clip
                )
                self.optimizer.step()
                self.optimizer.zero_grad(set_to_none=True)
            
            # Logging
            if step % self.config.log_every == 0 and step > 0:
                avg_loss = accumulated_loss / self.config.log_every
                elapsed = time.time() - start_time
                steps_per_sec = step / elapsed if elapsed > 0 else 0
                eta = (max_steps - step) / steps_per_sec / 3600 if steps_per_sec > 0 else 0
                
                self.losses.append(avg_loss)
                
                print(
                    f"  Step {step:6,} | Loss: {avg_loss:.4f} | "
                    f"LR: {lr:.2e} | Speed: {steps_per_sec:.1f} steps/s | "
                    f"ETA: {eta:.1f}h"
                )
                
                accumulated_loss = 0.0
            
            # Save checkpoint
            if step > 0 and step % self.config.save_every == 0:
                self.save_checkpoint(step)
            
            # Evaluation
            if step > 0 and step % self.config.eval_every == 0:
                self.quick_eval()
        
        print(f"\n  ‚úì Training complete!")
        self.save_checkpoint(max_steps)
    
    def quick_eval(self):
        """Quick evaluation during training."""
        self.brain.eval()
        
        prompts = [
            "The capital of France is",
            "Once upon a time",
        ]
        
        print("\n  üìù Quick eval:")
        for prompt in prompts:
            generated = self.generate(prompt, max_tokens=20)
            print(f"     \"{prompt}\" ‚Üí \"{generated[len(prompt):].strip()[:40]}...\"")
        print()
        
        self.brain.train()
    
    @torch.no_grad()
    def generate(self, prompt: str, max_tokens: int = 50) -> str:
        """Generate text from prompt."""
        self.brain.eval()
        
        tokens = self.tokenizer.encode(prompt)
        input_ids = torch.tensor([tokens], device=self.device, dtype=torch.long)
        
        for _ in range(max_tokens):
            if self.config.use_bf16:
                with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
                    outputs = self.brain(input_ids)
            else:
                outputs = self.brain(input_ids)
            
            logits = outputs['logits']
            probs = F.softmax(logits[0, -1, :].float(), dim=-1)
            
            # Top-p sampling
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            cumsum = torch.cumsum(sorted_probs, dim=0)
            mask = cumsum < 0.9
            mask[0] = True
            filtered_probs = sorted_probs * mask.float()
            filtered_probs = filtered_probs / filtered_probs.sum()
            
            next_token = sorted_indices[torch.multinomial(filtered_probs, 1)]
            
            if next_token.item() == 0:  # EOS
                break
            
            input_ids = torch.cat(
                [input_ids, next_token.unsqueeze(0)], 
                dim=1
            )
        
        return self.tokenizer.decode(input_ids[0].tolist())
    
    def save_checkpoint(self, step: int):
        """Save training checkpoint."""
        Path(self.config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        
        # Get underlying model if compiled
        model_to_save = self.brain
        if hasattr(self.brain, '_orig_mod'):
            model_to_save = self.brain._orig_mod
        
        checkpoint = {
            'step': step,
            'model_state_dict': model_to_save.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'losses': self.losses,
            'config': self.config,
        }
        
        path = Path(self.config.checkpoint_dir) / f"nokai_1.8B_step_{step}.pt"
        torch.save(checkpoint, path)
        print(f"  üíæ Saved checkpoint: {path}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="N≈çkai v0.9 - 1.8B Standard Training"
    )
    
    parser.add_argument("--steps", type=int, default=100_000)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--checkpoint_dir", type=str, default="checkpoints_v09_1.8B")
    parser.add_argument("--no-bf16", action="store_true")
    parser.add_argument("--no-compile", action="store_true")
    
    args = parser.parse_args()
    
    config = Config1_8B(
        batch_size=args.batch_size,
        learning_rate=args.lr,
        total_steps=args.steps,
        checkpoint_dir=args.checkpoint_dir,
        use_bf16=not args.no_bf16,
        use_compile=not args.no_compile,
    )
    
    trainer = H100Trainer(config)
    
    if trainer.setup():
        trainer.train(args.steps)


if __name__ == "__main__":
    main()

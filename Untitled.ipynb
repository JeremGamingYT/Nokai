{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48d8988-e76d-4b2f-a93c-c3d8ad6a8e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION ---\n",
      "Device: cuda\n",
      "Mode: BIO-MAMBA (Lion + Compile + Dopamine)\n",
      "---------------------\n",
      "Mod√®le cr√©√© : 5.62 M params\n",
      "\n",
      "=== PHASE 1 : BIO-PRETRAINING ===\n",
      "Step 0 | Loss: 4.2965 | Dopamine: 1.00x | Speed: 53.1 it/s\n",
      "Step 100 | Loss: 2.1027 | Dopamine: 2.00x | Speed: 1.0 it/s\n",
      "Step 200 | Loss: 1.5849 | Dopamine: 2.00x | Speed: 1.0 it/s\n",
      "Step 300 | Loss: 1.3966 | Dopamine: 2.00x | Speed: 1.0 it/s\n",
      "Step 400 | Loss: 1.3228 | Dopamine: 2.00x | Speed: 1.0 it/s\n",
      "\n",
      "=== G√âN√âRATION DATASET INSTRUCT ===\n",
      "\n",
      "=== PHASE 2 : FINE-TUNING (SFT) ===\n",
      "SFT Step 0 | Loss: 2.8113\n",
      "SFT Step 50 | Loss: 0.3339\n",
      "SFT Step 100 | Loss: 0.1407\n",
      "SFT Step 150 | Loss: 0.1013\n",
      "SFT Step 200 | Loss: 0.0945\n",
      "SFT Step 250 | Loss: 0.0874\n",
      "\n",
      "=== TEST FINAL (Temp√©rature activ√©e) ===\n",
      "User: Who are you?\n",
      "Bot :  I am NanoMamba.\n",
      "--------------------\n",
      "User: Good morrow\n",
      "Bot :  Hello friend.\n",
      "--------------------\n",
      "User: Tell me a story\n",
      "Bot :  The CPU hummed.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# --- CONFIGURATION (BIO-ACCELERATED) ---\n",
    "class Config:\n",
    "    block_size = 256        \n",
    "    vocab_size = 65         \n",
    "    n_layer = 6             # Profondeur d√©cente\n",
    "    n_embd = 384            # Largeur confortable (Lion √©conomise la RAM, on peut monter !)\n",
    "    d_state = 16            \n",
    "    d_conv = 4              \n",
    "    expand = 2              \n",
    "    \n",
    "    # Hyperparam√®tres Bio-Inspir√©s\n",
    "    lr_base = 5e-5          # Lion a besoin d'un LR plus bas qu'Adam (10x plus bas souvent)\n",
    "    weight_decay = 1e-2\n",
    "    \n",
    "    iters_pretrain = 500   # Shakespeare\n",
    "    iters_sft = 300         # Dialogue\n",
    "    \n",
    "    batch_size = 48         # Augment√© gr√¢ce √† Lion\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Config()\n",
    "torch.set_default_device(config.device)\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print(f\"--- CONFIGURATION ---\\nDevice: {config.device}\\nMode: BIO-MAMBA (Lion + Compile + Dopamine)\\n---------------------\")\n",
    "\n",
    "# --- 1. L'OPTIMISEUR \"LION\" (Biologically Inspired) ---\n",
    "# Remplace AdamW. Utilise le SIGNE du gradient (+1/-1), imitant une activation neuronale binaire/ternaire.\n",
    "# Plus rapide, moins de m√©moire.\n",
    "class Lion(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None: loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None: continue\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse: raise RuntimeError('Lion does not support sparse gradients')\n",
    "                \n",
    "                state = self.state[p]\n",
    "                if len(state) == 0: state['exp_avg'] = torch.zeros_like(p)\n",
    "                \n",
    "                exp_avg = state['exp_avg']\n",
    "                beta1, beta2 = group['betas']\n",
    "                \n",
    "                # Update = Momentum * beta1 + Grad * (1-beta1)\n",
    "                update = exp_avg * beta1 + grad * (1 - beta1)\n",
    "                \n",
    "                # Mise √† jour des poids avec le SIGNE (Ternaire: -1, 0, 1)\n",
    "                # p = p - lr * (sign(update) + weight_decay * p)\n",
    "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "                p.data.add_(torch.sign(update), alpha=-group['lr']) # <--- C'est ici la magie\n",
    "                \n",
    "                # Mise √† jour momentum\n",
    "                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n",
    "        return loss\n",
    "\n",
    "# --- 2. LE MOTEUR MATH√âMATIQUE (PSCAN) ---\n",
    "def pscan(A, B, C, D, x, delta):\n",
    "    # La boucle est lente en Python, MAIS torch.compile va la transformer en kernel GPU rapide !\n",
    "    bs, dim, seq_len = x.shape\n",
    "    n_state = A.shape[1]\n",
    "    \n",
    "    h = torch.zeros(bs, dim, n_state, device=x.device)\n",
    "    y = torch.zeros_like(x)\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        xt = x[:, :, t].unsqueeze(-1)       \n",
    "        dt = delta[:, :, t].unsqueeze(-1)   \n",
    "        bt = B[:, :, t].unsqueeze(1)        \n",
    "        ct = C[:, :, t].unsqueeze(1)        \n",
    "        \n",
    "        # Discr√©tisation ZOH simplifi√©e\n",
    "        dt_A = dt * A.unsqueeze(0) \n",
    "        A_bar = torch.exp(dt_A) \n",
    "        B_bar = bt * dt \n",
    "        \n",
    "        # √âtat r√©current\n",
    "        h = A_bar * h + B_bar * xt\n",
    "        \n",
    "        # Sortie\n",
    "        yt = (ct * h).sum(dim=-1) + D * xt.squeeze(-1)\n",
    "        y[:, :, t] = yt\n",
    "        \n",
    "    return y\n",
    "\n",
    "# --- 3. L'ARCHITECTURE ---\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.d_inner = config.n_embd * config.expand\n",
    "        self.dt_rank = config.d_state // 16 if config.d_state // 16 > 1 else 1\n",
    "\n",
    "        self.in_proj = nn.Linear(config.n_embd, self.d_inner * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner, out_channels=self.d_inner, bias=True,\n",
    "            kernel_size=config.d_conv, groups=self.d_inner, padding=config.d_conv - 1,\n",
    "        )\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + config.d_state * 2, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "\n",
    "        self.A_log = nn.Parameter(torch.log(torch.arange(1, config.d_state + 1, dtype=torch.float32)).repeat(self.d_inner, 1))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        self.out_proj = nn.Linear(self.d_inner, config.n_embd, bias=False)\n",
    "        self.act = nn.SiLU() \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        x_and_res = self.in_proj(x) \n",
    "        (x_ssm, res) = x_and_res.split(split_size=[self.d_inner, self.d_inner], dim=-1)\n",
    "        \n",
    "        x_ssm = x_ssm.transpose(1, 2)\n",
    "        x_ssm = self.conv1d(x_ssm)[:, :, :L]\n",
    "        x_ssm = x_ssm.transpose(1, 2)\n",
    "        x_ssm = self.act(x_ssm)\n",
    "        \n",
    "        x_dbl = self.x_proj(x_ssm) \n",
    "        (delta, B_val, C_val) = x_dbl.split([self.dt_rank, self.config.d_state, self.config.d_state], dim=-1)\n",
    "\n",
    "        delta = F.softplus(self.dt_proj(delta)) \n",
    "        A_continuous = -torch.exp(self.A_log) \n",
    "        \n",
    "        # Transpositions\n",
    "        scan_out = pscan(A_continuous, B_val.transpose(1, 2), C_val.transpose(1, 2), \n",
    "                         self.D, x_ssm.transpose(1, 2), delta.transpose(1, 2))\n",
    "        \n",
    "        out = scan_out.transpose(1, 2) * self.act(res)\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "class NanoMamba(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.layers = nn.ModuleList([MambaBlock(config) for _ in range(config.n_layer)])\n",
    "        self.norm_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "# --- 4. PR√âPARATION DONN√âES ---\n",
    "if not os.path.exists('input.txt'):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    data = requests.get(data_url).text\n",
    "    with open('input.txt', 'w') as f: f.write(data)\n",
    "else:\n",
    "    with open('input.txt', 'r') as f: data = f.read()\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data_tensor = torch.tensor(encode(data), dtype=torch.long)\n",
    "n = int(0.9*len(data_tensor))\n",
    "train_data = data_tensor[:n]\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(train_data) - config.block_size, (config.batch_size,))\n",
    "    x = torch.stack([train_data[i:i+config.block_size] for i in ix])\n",
    "    y = torch.stack([train_data[i+1:i+config.block_size+1] for i in ix])\n",
    "    return x.to(config.device), y.to(config.device)\n",
    "\n",
    "# --- 5. INITIALISATION & COMPILATION ---\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model = NanoMamba(config).to(config.device)\n",
    "print(f\"Mod√®le cr√©√© : {sum(p.numel() for p in model.parameters())/1e6:.2f} M params\")\n",
    "\n",
    "# --- CORRECTION : ON D√âSACTIVE LA COMPILATION ---\n",
    "# La boucle 'for' manuelle de pscan fait planter le compilateur.\n",
    "# Sur A100, le mode eager (normal) sera de toute fa√ßon tr√®s rapide.\n",
    "# model_compiled = torch.compile(model) <--- ON COMMENTE CETTE LIGNE\n",
    "\n",
    "optimizer = Lion(model.parameters(), lr=config.lr_base, weight_decay=config.weight_decay)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# --- 6. TRAINING LOOP: \"DOPAMINE STYLE\" ---\n",
    "print(\"\\n=== PHASE 1 : BIO-PRETRAINING ===\")\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "running_loss = 0.0\n",
    "\n",
    "for iter in range(config.iters_pretrain):\n",
    "    xb, yb = get_batch()\n",
    "    \n",
    "    # 1. Forward (Sans 'model_compiled', on utilise 'model' direct)\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        logits, loss = model(xb, yb) \n",
    "    \n",
    "    # 2. M√©canisme de Dopamine (Surprise)\n",
    "    loss_val = loss.item()\n",
    "    if iter > 50:\n",
    "        avg_loss = running_loss / 50\n",
    "        surprise = (loss_val / avg_loss) ** 2 \n",
    "        surprise = min(max(surprise, 0.5), 2.0)\n",
    "    else:\n",
    "        surprise = 1.0\n",
    "        \n",
    "    running_loss = running_loss * 0.98 + loss_val * 0.02 if iter > 0 else loss_val\n",
    "\n",
    "    # 3. Backward \"Dop√©\"\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss * surprise).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        dt = time.time() - start_time\n",
    "        ips = 100 / (dt + 1e-6)\n",
    "        start_time = time.time()\n",
    "        print(f\"Step {iter} | Loss: {loss_val:.4f} | Dopamine: {surprise:.2f}x | Speed: {ips:.1f} it/s\")\n",
    "\n",
    "# --- 7. PHASE 2 : DATASET SFT √âTENDU ---\n",
    "print(\"\\n=== G√âN√âRATION DATASET INSTRUCT ===\")\n",
    "# On g√©n√®re un dataset vari√© pour √©viter l'apprentissage par coeur\n",
    "identities = [\"I am NanoMamba.\", \"A digital spirit.\", \"Code made manifest.\"]\n",
    "greetings_u = [\"Hello\", \"Hi\", \"Greetings\", \"Yo\", \"Good morrow\"]\n",
    "greetings_b = [\"Hail!\", \"Greetings.\", \"Hello friend.\", \"At your service.\"]\n",
    "stories = [\"The King fell.\", \"A star died.\", \"The CPU hummed.\"]\n",
    "\n",
    "dialogues = []\n",
    "# Mix al√©atoire\n",
    "for _ in range(100):\n",
    "    dialogues.append((\"Who are you?\", random.choice(identities)))\n",
    "    dialogues.append((random.choice(greetings_u), random.choice(greetings_b)))\n",
    "    dialogues.append((\"Tell me a story\", random.choice(stories)))\n",
    "    \n",
    "random.shuffle(dialogues)\n",
    "sft_text = \"\".join([f\"User: {u}\\nBot: {b}\\n\" for u, b in dialogues])\n",
    "sft_tensor = torch.tensor(encode(sft_text), dtype=torch.long)\n",
    "\n",
    "def get_sft_batch():\n",
    "    ix = torch.randint(len(sft_tensor) - config.block_size, (config.batch_size,))\n",
    "    x = torch.stack([sft_tensor[i:i+config.block_size] for i in ix])\n",
    "    y = torch.stack([sft_tensor[i+1:i+config.block_size+1] for i in ix])\n",
    "    return x.to(config.device), y.to(config.device)\n",
    "\n",
    "# --- 8. FINE-TUNING RAPIDE (CORRIG√â) ---\n",
    "print(\"\\n=== PHASE 2 : FINE-TUNING (SFT) ===\")\n",
    "# On garde Lion mais on baisse le LR pour affiner sans casser\n",
    "for g in optimizer.param_groups: g['lr'] = config.lr_base * 0.2\n",
    "\n",
    "for iter in range(config.iters_sft):\n",
    "    xb, yb = get_sft_batch()\n",
    "    \n",
    "    with torch.amp.autocast('cuda'):\n",
    "        # CORRECTION ICI : On utilise 'model' et non 'model_compiled'\n",
    "        logits, loss = model(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if iter % 50 == 0: \n",
    "        print(f\"SFT Step {iter} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --- 9. CHAT FINAL ---\n",
    "print(\"\\n=== TEST FINAL (Temp√©rature activ√©e) ===\")\n",
    "def chat(prompt, temp=0.7):\n",
    "    model.eval()\n",
    "    full_prompt = f\"User: {prompt}\\nBot:\"\n",
    "    input_ids = torch.tensor(encode(full_prompt), dtype=torch.long, device=config.device).unsqueeze(0)\n",
    "    generated = []\n",
    "    \n",
    "    for _ in range(150):\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(input_ids) # Pas besoin de compile pour l'inf√©rence simple\n",
    "        \n",
    "        logits = logits[:, -1, :] / temp\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        if idx_next.item() == stoi['\\n']: break\n",
    "        input_ids = torch.cat((input_ids, idx_next), dim=1)\n",
    "        generated.append(idx_next.item())\n",
    "        \n",
    "    return decode(generated)\n",
    "\n",
    "tests = [\"Who are you?\", \"Good morrow\", \"Tell me a story\"]\n",
    "for t in tests:\n",
    "    print(f\"User: {t}\")\n",
    "    print(f\"Bot : {chat(t)}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5214f8-487e-4015-a4d3-c33848aefc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee838b24-b7ec-42f0-879c-f8ea0ffb23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Fichier de sauvegarde d√©tect√©, mais par s√©curit√© (apr√®s un crash), on repart √† z√©ro.\n",
      "\n",
      "=== PHASE 1 : BIO-PRETRAINING V3 (Stabilis√©) ===\n",
      "Step 0 | Loss: 4.2965 | Surprise: 1.00x | Speed: 88.4 it/s\n",
      "Step 100 | Loss: 2.0870 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 200 | Loss: 1.6007 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 300 | Loss: 1.4095 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 400 | Loss: 1.2944 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 500 | Loss: 1.2456 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 600 | Loss: 1.1350 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 700 | Loss: 1.0912 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 800 | Loss: 1.0132 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 900 | Loss: 0.9487 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1000 | Loss: 0.8719 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1100 | Loss: 0.8449 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1200 | Loss: 0.7711 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1300 | Loss: 0.7136 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1400 | Loss: 0.6642 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "‚ö†Ô∏è Alerte: Loss NaN √† l'it√©ration 1459. Skip.\n",
      "Step 1500 | Loss: 0.5353 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "‚ö†Ô∏è Alerte: Loss NaN √† l'it√©ration 1513. Skip.\n",
      "Step 1600 | Loss: 0.5859 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1700 | Loss: 0.5448 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1800 | Loss: 0.4599 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "Step 1900 | Loss: 0.4396 | Surprise: 2.00x | Speed: 1.0 it/s\n",
      "\n",
      "=== PHASE 2 : SFT ===\n",
      "SFT Step 0 | Loss: 5.4433\n",
      "SFT Step 50 | Loss: 0.0590\n",
      "SFT Step 100 | Loss: 0.0466\n",
      "SFT Step 150 | Loss: 0.0421\n",
      "SFT Step 200 | Loss: 0.0428\n",
      "SFT Step 250 | Loss: 0.0411\n",
      "\n",
      "=== CHAT V3 (Alive) ===\n",
      "User: Who are you?\n",
      "Bot :  I am Bio-Mamba.\n",
      "User: Tell me a story\n",
      "Bot :  The stars sang.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# --- CONFIGURATION (V3: ROBUSTESSE) ---\n",
    "class Config:\n",
    "    block_size = 256        \n",
    "    vocab_size = 65         \n",
    "    n_layer = 6             \n",
    "    n_embd = 384            \n",
    "    d_state = 16            \n",
    "    d_conv = 4              \n",
    "    expand = 2              \n",
    "    \n",
    "    lr_base = 5e-5          \n",
    "    weight_decay = 1e-2\n",
    "    \n",
    "    # S√©curit√©s V3\n",
    "    gradient_noise = 0.001  # R√©duit (0.01 √©tait trop violent pour FP16)\n",
    "    max_grad_norm = 1.0     \n",
    "    \n",
    "    iters_pretrain = 2000   \n",
    "    iters_sft = 300         \n",
    "    \n",
    "    batch_size = 48         \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    save_path = \"nano_mamba_brain.pth\"\n",
    "\n",
    "config = Config()\n",
    "torch.set_default_device(config.device)\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# --- 1. OPTIMISEUR LION (STABILIS√â) ---\n",
    "class Lion(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None: loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None: continue\n",
    "                grad = p.grad\n",
    "                \n",
    "                # S√âCURIT√â 1 : Nettoyage des Gradients NaN/Inf\n",
    "                if torch.isnan(grad).any() or torch.isinf(grad).any():\n",
    "                    grad.zero_() # On ignore ce neurone s'il bug\n",
    "                \n",
    "                # Bruit Thermique (Doux)\n",
    "                if config.gradient_noise > 0:\n",
    "                    noise = torch.randn_like(grad) * config.gradient_noise * group['lr']\n",
    "                    grad = grad + noise\n",
    "                \n",
    "                state = self.state[p]\n",
    "                if len(state) == 0: state['exp_avg'] = torch.zeros_like(p)\n",
    "                \n",
    "                exp_avg = state['exp_avg']\n",
    "                beta1, beta2 = group['betas']\n",
    "                \n",
    "                update = exp_avg * beta1 + grad * (1 - beta1)\n",
    "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "                p.data.add_(torch.sign(update), alpha=-group['lr']) \n",
    "                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n",
    "        return loss\n",
    "\n",
    "# --- 2. NOYAU PSCAN (Inchang√©) ---\n",
    "def pscan(A, B, C, D, x, delta):\n",
    "    bs, dim, seq_len = x.shape\n",
    "    n_state = A.shape[1]\n",
    "    h = torch.zeros(bs, dim, n_state, device=x.device)\n",
    "    y = torch.zeros_like(x)\n",
    "    for t in range(seq_len):\n",
    "        xt = x[:, :, t].unsqueeze(-1)       \n",
    "        dt = delta[:, :, t].unsqueeze(-1)   \n",
    "        bt = B[:, :, t].unsqueeze(1)        \n",
    "        ct = C[:, :, t].unsqueeze(1)        \n",
    "        dt_A = dt * A.unsqueeze(0) \n",
    "        A_bar = torch.exp(dt_A) \n",
    "        B_bar = bt * dt \n",
    "        h = A_bar * h + B_bar * xt\n",
    "        yt = (ct * h).sum(dim=-1) + D * xt.squeeze(-1)\n",
    "        y[:, :, t] = yt\n",
    "    return y\n",
    "\n",
    "# --- 3. MAMBA BLOCK (Inchang√©) ---\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.d_inner = config.n_embd * config.expand\n",
    "        self.dt_rank = config.d_state // 16 if config.d_state // 16 > 1 else 1\n",
    "        self.in_proj = nn.Linear(config.n_embd, self.d_inner * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, bias=True, kernel_size=config.d_conv, groups=self.d_inner, padding=config.d_conv - 1)\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + config.d_state * 2, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "        self.A_log = nn.Parameter(torch.log(torch.arange(1, config.d_state + 1, dtype=torch.float32)).repeat(self.d_inner, 1))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        self.out_proj = nn.Linear(self.d_inner, config.n_embd, bias=False)\n",
    "        self.act = nn.SiLU() \n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        x_and_res = self.in_proj(x) \n",
    "        (x_ssm, res) = x_and_res.split(split_size=[self.d_inner, self.d_inner], dim=-1)\n",
    "        x_ssm = x_ssm.transpose(1, 2)\n",
    "        x_ssm = self.conv1d(x_ssm)[:, :, :L]\n",
    "        x_ssm = x_ssm.transpose(1, 2)\n",
    "        x_ssm = self.act(x_ssm)\n",
    "        x_dbl = self.x_proj(x_ssm) \n",
    "        (delta, B_val, C_val) = x_dbl.split([self.dt_rank, self.config.d_state, self.config.d_state], dim=-1)\n",
    "        delta = F.softplus(self.dt_proj(delta)) \n",
    "        scan_out = pscan(-torch.exp(self.A_log), B_val.transpose(1, 2), C_val.transpose(1, 2), self.D, x_ssm.transpose(1, 2), delta.transpose(1, 2))\n",
    "        out = scan_out.transpose(1, 2) * self.act(res)\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "class NanoMamba(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.layers = nn.ModuleList([MambaBlock(config) for _ in range(config.n_layer)])\n",
    "        self.norm_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "        for layer in self.layers: x = x + layer(x)\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "# --- 4. DATA & UTILS ---\n",
    "if not os.path.exists('input.txt'):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open('input.txt', 'w') as f: f.write(requests.get(data_url).text)\n",
    "with open('input.txt', 'r') as f: data = f.read()\n",
    "chars = sorted(list(set(data)))\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s if c in stoi] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "data_tensor = torch.tensor(encode(data), dtype=torch.long)\n",
    "train_data = data_tensor[:int(0.9*len(data_tensor))]\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(train_data) - config.block_size, (config.batch_size,))\n",
    "    x = torch.stack([train_data[i:i+config.block_size] for i in ix])\n",
    "    y = torch.stack([train_data[i+1:i+config.block_size+1] for i in ix])\n",
    "    return x.to(config.device), y.to(config.device)\n",
    "\n",
    "# --- 5. INITIALISATION (Avec Reset si corrompu) ---\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model = NanoMamba(config).to(config.device)\n",
    "optimizer = Lion(model.parameters(), lr=config.lr_base, weight_decay=config.weight_decay)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# On supprime le fichier s'il existe pour repartir de z√©ro (puisque le dernier a crash√©)\n",
    "if os.path.exists(config.save_path):\n",
    "    print(\"‚ö†Ô∏è Fichier de sauvegarde d√©tect√©, mais par s√©curit√© (apr√®s un crash), on repart √† z√©ro.\")\n",
    "    # Si tu veux vraiment charger, commente la ligne ci-dessous\n",
    "    # checkpoint = torch.load(config.save_path); model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# --- 6. TRAINING ROBUSTE ---\n",
    "print(\"\\n=== PHASE 1 : BIO-PRETRAINING V3 (Stabilis√©) ===\")\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "running_loss = 0.0\n",
    "\n",
    "for iter in range(config.iters_pretrain):\n",
    "    xb, yb = get_batch()\n",
    "    \n",
    "    with torch.amp.autocast('cuda'):\n",
    "        logits, loss = model(xb, yb)\n",
    "    \n",
    "    # --- S√âCURIT√â 2 : Calcul Dopamine sans Crash ---\n",
    "    loss_val = loss.item()\n",
    "    \n",
    "    # Si la loss est NaN, on skip l'it√©ration (protection vitale)\n",
    "    if math.isnan(loss_val) or math.isinf(loss_val):\n",
    "        print(f\"‚ö†Ô∏è Alerte: Loss NaN √† l'it√©ration {iter}. Skip.\")\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        continue\n",
    "\n",
    "    if iter > 50:\n",
    "        avg_loss = max(running_loss / 50, 1e-6) # Emp√™che division par 0\n",
    "        surprise = (loss_val / avg_loss) ** 2 \n",
    "        \n",
    "        # Clamp de la surprise (Bornes strictes)\n",
    "        if math.isnan(surprise) or math.isinf(surprise):\n",
    "            surprise = 1.0\n",
    "        else:\n",
    "            surprise = min(max(surprise, 0.5), 2.0)\n",
    "    else:\n",
    "        surprise = 1.0\n",
    "        \n",
    "    running_loss = running_loss * 0.98 + loss_val * 0.02 if iter > 0 else loss_val\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss * surprise).backward()\n",
    "    \n",
    "    # Hom√©ostasie\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        dt = time.time() - start_time\n",
    "        ips = 100 / (dt + 1e-6)\n",
    "        start_time = time.time()\n",
    "        print(f\"Step {iter} | Loss: {loss_val:.4f} | Surprise: {surprise:.2f}x | Speed: {ips:.1f} it/s\")\n",
    "\n",
    "# Sauvegarde Propre\n",
    "torch.save({'model_state_dict': model.state_dict()}, config.save_path)\n",
    "\n",
    "# --- 7. PHASE 2 : SFT (S√ªr) ---\n",
    "print(\"\\n=== PHASE 2 : SFT ===\")\n",
    "dialogues = []\n",
    "base_pairs = [(\"Who are you?\", \"I am Bio-Mamba.\"), (\"Hello\", \"Greetings friend.\"), (\"Tell me a story\", \"The stars sang.\"), (\"Are you alive?\", \"I learn, so I am.\")]\n",
    "for _ in range(50):\n",
    "    for q, a in base_pairs: dialogues.append((q, a))\n",
    "random.shuffle(dialogues)\n",
    "sft_text = \"\".join([f\"User: {u}\\nBot: {b}\\n\" for u, b in dialogues])\n",
    "sft_tensor = torch.tensor(encode(sft_text), dtype=torch.long)\n",
    "\n",
    "def get_sft_batch():\n",
    "    ix = torch.randint(len(sft_tensor) - config.block_size, (config.batch_size,))\n",
    "    x = torch.stack([sft_tensor[i:i+config.block_size] for i in ix])\n",
    "    y = torch.stack([sft_tensor[i+1:i+config.block_size+1] for i in ix])\n",
    "    return x.to(config.device), y.to(config.device)\n",
    "\n",
    "for g in optimizer.param_groups: g['lr'] = config.lr_base * 0.2\n",
    "\n",
    "for iter in range(config.iters_sft):\n",
    "    xb, yb = get_sft_batch()\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        logits, loss = model(xb, yb)\n",
    "    \n",
    "    if math.isnan(loss.item()): continue # Protection SFT\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if iter % 50 == 0: print(f\"SFT Step {iter} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --- 8. CHAT ---\n",
    "print(\"\\n=== CHAT V3 (Alive) ===\")\n",
    "def chat(prompt, temp=0.7):\n",
    "    model.eval()\n",
    "    full_prompt = f\"User: {prompt}\\nBot:\"\n",
    "    input_ids = torch.tensor(encode(full_prompt), dtype=torch.long, device=config.device).unsqueeze(0)\n",
    "    generated = []\n",
    "    for _ in range(150):\n",
    "        with torch.no_grad(): logits, _ = model(input_ids)\n",
    "        logits = logits[:, -1, :] / temp\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        if idx_next.item() == stoi['\\n']: break\n",
    "        input_ids = torch.cat((input_ids, idx_next), dim=1)\n",
    "        generated.append(idx_next.item())\n",
    "    return decode(generated)\n",
    "\n",
    "tests = [\"Who are you?\", \"Tell me a story\"]\n",
    "for t in tests:\n",
    "    print(f\"User: {t}\")\n",
    "    print(f\"Bot : {chat(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0e0b7-5543-4d3b-8d23-8591b1ce542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation de l'architecture...\n",
      "Chargement du cerveau sauvegard√©...\n",
      "Cerveau Bio-Mamba actif et pr√™t ! üß†\n",
      "\n",
      "--- D√âBUT DE LA SESSION (Tapez 'exit' pour quitter) ---\n",
      "Bot: I am awake. Speak.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  I see, with despised arms?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  Juliet ere you go to bed,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Juliet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  ay i but not a greater soldier than he hath forced me to be the fruit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is 2+2 ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  Warwick, thou art a coward,\n"
     ]
    }
   ],
   "source": [
    "# --- MODE INTERACTIF : CHARGEMENT ET DISCUSSION ---\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. On recr√©e la structure vide\n",
    "print(\"Initialisation de l'architecture...\")\n",
    "model_inference = NanoMamba(config).to(config.device)\n",
    "\n",
    "# 2. On charge les poids sauvegard√©s\n",
    "if os.path.exists(\"nano_mamba_brain.pth\"):\n",
    "    print(\"Chargement du cerveau sauvegard√©...\")\n",
    "    checkpoint = torch.load(\"nano_mamba_brain.pth\", map_location=config.device)\n",
    "    model_inference.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Cerveau Bio-Mamba actif et pr√™t ! üß†\")\n",
    "else:\n",
    "    print(\"Erreur : Aucun cerveau trouv√© !\")\n",
    "\n",
    "# 3. Fonction de Chat en boucle\n",
    "def live_chat():\n",
    "    model_inference.eval()\n",
    "    print(\"\\n--- D√âBUT DE LA SESSION (Tapez 'exit' pour quitter) ---\")\n",
    "    print(\"Bot: I am awake. Speak.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Bot: Rest mode activated.\")\n",
    "            break\n",
    "            \n",
    "        # Formatage SFT\n",
    "        prompt = f\"User: {user_input}\\nBot:\"\n",
    "        input_ids = torch.tensor(encode(prompt), dtype=torch.long, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        generated = []\n",
    "        # On g√©n√®re max 200 tokens\n",
    "        for _ in range(200):\n",
    "            with torch.no_grad():\n",
    "                logits, _ = model_inference(input_ids)\n",
    "            \n",
    "            # Temperature : 0.6 pour √™tre pr√©cis mais pas robotique\n",
    "            logits = logits[:, -1, :] / 0.6\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Stop si retour √† la ligne\n",
    "            if idx_next.item() == stoi['\\n']:\n",
    "                break\n",
    "                \n",
    "            input_ids = torch.cat((input_ids, idx_next), dim=1)\n",
    "            generated.append(idx_next.item())\n",
    "            \n",
    "        response = decode(generated)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Lancer le chat\n",
    "live_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8df93-6d54-4e1d-9cbf-30b7a270aabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

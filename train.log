======================================================================
NŌKAI COGNITIVE TRAINING V2.1 - ALIGNED BRAIN
======================================================================
  Preset: nano
  BPE Tokenization: True
  Hebbian Learning: True
  Dopamine Gating: True
======================================================================

[1/5] Loading data...
  Loading from file: data/tinystories.txt
  Loaded 4991 texts from file

[2/5] Setting up tokenizer (DATA-ALIGNED)...
  Training NEW BPE tokenizer on data...
[NokaiTokenizer] Training BPE with vocab_size=32000...
[00:00:00] Tokenize words                 ██████████████████ 5093     /     5093
[00:00:00] Count pairs                    ██████████████████ 5093     /     5093
[00:00:01] Compute merges                 ██████████████████ 5735     /     5735
[NokaiTokenizer] Training complete! Vocab size: 5996
[NokaiTokenizer] Saved to checkpoints/tokenizer.json
  Tokenizer trained! Vocab size: 5996

[3/5] Creating NeuromorphicBrain (nano)...
  ⚡ ALIGNED: Using tokenizer vocab_size = 5996
============================================================
NŌKAI NEUROMORPHIC BRAIN
============================================================
  Total parameters: 23,670,255
  Trainable: 23,440,879
  Modules:
    ├── Thalamus (Gateway)
    ├── Cortex (Processing)
    ├── Working Memory (PFC)
    ├── Episodic Memory (Hippocampus)
    ├── Semantic Memory (Neocortex)
    ├── Dopamine Circuit (VTA)
    ├── Striatum (Decisions)
    ├── dACC (Metacognition)
    ├── Attention Controller
    └── Oscillations
============================================================
  Parameters: 23,670,255
  Embedding dim: 128
  Vocab size: 5996 (ALIGNED with tokenizer)

[4/5] Creating dataset...
[Dataset] Loaded 4991 valid texts
  Samples: 4991
  Batches per epoch: 624

[5/5] Initializing trainer...
  Hebbian Integrators: 1344

[CognitiveTrainer] Initialized
  Device: cuda
  Mixed Precision: True
  Hebbian Learning: True
  Dopamine Gating: True

[TRAINING] Starting...
  Epochs: 5
  Steps per epoch: 624
  Total steps: 3,120
  Consolidation every: 500 steps
======================================================================

Epoch 1/5:   0%|                                        | 0/624 [00:00<?, ?it/s]/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Epoch 1/5:  80%|▊| 499/624 [05:53<01:02,  2.01it/s, loss=1.9866, DA=0.36, surp=0
==================================================
CONSOLIDATION (Sleep) - Step 500
==================================================
  Consolidated: 960
  Pruned: 0
  Hebbian updates: 336000
  Avg Dopamine: 0.503
  Avg Surprise: 0.319
==================================================

Epoch 1/5: 100%|█| 624/624 [06:49<00:00,  1.52it/s, loss=1.0650, DA=0.37, surp=0

 Epoch 1 Complete:
  Average Loss: 3.5705
  Dopamine: 0.503
  Surprise: 0.258
  Hebbian Updates: 419,328
======================================================================
NÅŒKAI COGNITIVE TRAINING V2.1 - ALIGNED BRAIN
======================================================================
  Preset: nano
  BPE Tokenization: True
  Hebbian Learning: True
  Dopamine Gating: True
======================================================================

[1/5] Loading data...
  Loading from file: /workspace/data/tinystories.txt
  Loaded 4991 texts from file

[2/5] Setting up tokenizer (DATA-ALIGNED)...
  Loading existing BPE tokenizer...
[NokaiTokenizer] Loaded from checkpoints/tokenizer.json (vocab_size=5996)
  Tokenizer vocab: 5996

[3/5] Creating NeuromorphicBrain (nano)...
  âš¡ ALIGNED: Using tokenizer vocab_size = 5996
============================================================
NÅŒKAI NEUROMORPHIC BRAIN
============================================================
  Total parameters: 23,670,255
  Trainable: 23,440,879
  Modules:
    â”œâ”€â”€ Thalamus (Gateway)
    â”œâ”€â”€ Cortex (Processing)
    â”œâ”€â”€ Working Memory (PFC)
    â”œâ”€â”€ Episodic Memory (Hippocampus)
    â”œâ”€â”€ Semantic Memory (Neocortex)
    â”œâ”€â”€ Dopamine Circuit (VTA)
    â”œâ”€â”€ Striatum (Decisions)
    â”œâ”€â”€ dACC (Metacognition)
    â”œâ”€â”€ Attention Controller
    â””â”€â”€ Oscillations
============================================================
  Parameters: 23,670,255
  Embedding dim: 128
  Vocab size: 5996 (ALIGNED with tokenizer)

[4/5] Creating dataset...
[Dataset] Loaded 4991 valid texts
  Samples: 4991
  Batches per epoch: 624

[5/5] Initializing trainer...
  Hebbian Integrators: 1344

[CognitiveTrainer] Initialized
  Device: cuda
  Mixed Precision: True
  Hebbian Learning: True
  Dopamine Gating: True

[TRAINING] Starting...
  Epochs: 5
  Steps per epoch: 624
  Total steps: 3,120
  Consolidation every: 500 steps
======================================================================

Epoch 1/5:   0%|                                        | 0/624 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Epoch 1/5:  80%|â–Š| 499/624 [04:02<00:42,  2.95it/s, loss=2.0447, DA=0.36, surp=0
==================================================
CONSOLIDATION (Sleep) - Step 500
==================================================
  Consolidated: 960
  Pruned: 0
  Hebbian updates: 336000
  Avg Dopamine: 0.503
  Avg Surprise: 0.318
==================================================

Epoch 1/5: 100%|â–ˆ| 624/624 [04:41<00:00,  2.22it/s, loss=1.1219, DA=0.37, surp=0

 Epoch 1 Complete:
  Average Loss: 3.5818
  Dopamine: 0.502
  Surprise: 0.268
  Hebbian Updates: 419,328
Epoch 2/5:  60%|â–Œ| 375/624 [01:50<01:16,  3.27it/s, loss=0.9217, DA=0.43, surp=0
==================================================
CONSOLIDATION (Sleep) - Step 1000
==================================================
  Consolidated: 960
  Pruned: 0
  Hebbian updates: 672000
  Avg Dopamine: 0.498
  Avg Surprise: 0.197
==================================================

  New best loss: 0.9372
Epoch 2/5: 100%|â–ˆ| 624/624 [03:06<00:00,  3.35it/s, loss=0.8248, DA=0.46, surp=0

 Epoch 2 Complete:
  Average Loss: 0.9731
  Dopamine: 0.499
  Surprise: 0.156
  Hebbian Updates: 838,656
Epoch 3/5:  40%|â–| 251/624 [01:13<01:57,  3.17it/s, loss=0.6528, DA=0.38, surp=0
==================================================
CONSOLIDATION (Sleep) - Step 1500
==================================================
  Consolidated: 960
  Pruned: 0
  Hebbian updates: 1008000
  Avg Dopamine: 0.500
  Avg Surprise: 0.168
==================================================

Epoch 3/5: 100%|â–ˆ| 624/624 [03:01<00:00,  3.43it/s, loss=0.8704, DA=0.41, surp=0

 Epoch 3 Complete:
  Average Loss: 0.7468
  Dopamine: 0.499
  Surprise: 0.128
  Hebbian Updates: 1,257,984
Epoch 4/5:  20%|â–| 127/624 [00:37<02:32,  3.26it/s, loss=0.5938, DA=0.48, surp=0
==================================================
CONSOLIDATION (Sleep) - Step 2000
==================================================
  Consolidated: 960
  Pruned: 0
  Hebbian updates: 1344000
  Avg Dopamine: 0.499
  Avg Surprise: 0.114
==================================================

  New best loss: 0.6876
Epoch 4/5: 100%|â–ˆ| 624/624 [03:06<00:00,  3.34it/s, loss=0.6538, DA=0.53, surp=0

 Epoch 4 Complete:
  Average Loss: 0.6903
  Dopamine: 0.502
  Surprise: 0.125
  Hebbian Updates: 1,677,312
Epoch 5/5:   0%| | 3/624 [00:01<04:27,  2.32it/s, loss=0.8577, DA=0.38, surp=0.3
==================================================
CONSOLIDATION (Sleep) - Step 2500
==================================================
  Consolidated: 960
  Pruned: 0
  Hebbian updates: 1680000
  Avg Dopamine: 0.500
  Avg Surprise: 0.127
==================================================

Epoch 5/5:  81%|â–Š| 503/624 [02:28<00:38,  3.17it/s, loss=0.6970, DA=0.59, surp=0
==================================================
CONSOLIDATION (Sleep) - Step 3000
==================================================
  Consolidated: 960
  Pruned: 0
  Hebbian updates: 2016000
  Avg Dopamine: 0.495
  Avg Surprise: 0.125
==================================================

  New best loss: 0.6608
Epoch 5/5: 100%|â–ˆ| 624/624 [03:07<00:00,  3.33it/s, loss=0.6054, DA=0.40, surp=0

 Epoch 5 Complete:
  Average Loss: 0.6723
  Dopamine: 0.502
  Surprise: 0.115
  Hebbian Updates: 2,096,640

======================================================================
TRAINING COMPLETE
======================================================================
  Total time: 0.29 hours
  Total steps: 3,120
  Hebbian updates: 2,096,640
  Consolidations: 6
  Best loss: 0.6608
  Final Dopamine: 0.502
  Model saved to: checkpoints/brain_best.pt
======================================================================

[Loading] Checkpoint: /workspace/checkpoints/brain_epoch_5.pt
  Inferred config: vocab=5996, dim=128, seq_len=512
============================================================
NÅŒKAI NEUROMORPHIC BRAIN
============================================================
  Total parameters: 23,670,255
  Trainable: 23,440,879
  Modules:
    â”œâ”€â”€ Thalamus (Gateway)
    â”œâ”€â”€ Cortex (Processing)
    â”œâ”€â”€ Working Memory (PFC)
    â”œâ”€â”€ Episodic Memory (Hippocampus)
    â”œâ”€â”€ Semantic Memory (Neocortex)
    â”œâ”€â”€ Dopamine Circuit (VTA)
    â”œâ”€â”€ Striatum (Decisions)
    â”œâ”€â”€ dACC (Metacognition)
    â”œâ”€â”€ Attention Controller
    â””â”€â”€ Oscillations
============================================================
  Loaded weights successfully
  Parameters: 23,670,255
[Loading] Tokenizer: /workspace/checkpoints/tokenizer.json
[NokaiTokenizer] Loaded from /workspace/checkpoints/tokenizer.json (vocab_size=5996)
  BPE tokenizer loaded (vocab=5996)

============================================================
ðŸ”¬ NÅŒKAI BRAIN ANALYSIS
============================================================

[Dopamine System]
  current_tonic: 0.5026
  current_phasic: 0.1248
  current_rpe: 1.0000
  current_novelty: 1.0000
  avg_reward: -0.0126
  habituation: 1.0000
  rpe_mean: -0.0112
  rpe_std: 0.8016
  da_mean: 0.5041
  da_std: 0.1025

[Model Architecture]
  Total parameters: 23,670,255
  Trainable parameters: 23,440,879
  Embedding dim: 128
  Vocab size: 5996
  Max sequence length: 512

[Cortex]
  Layers: 3
  Total columns: 448

[Memory Usage]
  Model size: 90.29 MB
  GPU memory allocated: 191.28 MB
  GPU memory cached: 478.00 MB
============================================================

[Loading] Checkpoint: /workspace/checkpoints/brain_epoch_5.pt
  Inferred config: vocab=5996, dim=128, seq_len=512
============================================================
NÅŒKAI NEUROMORPHIC BRAIN
============================================================
  Total parameters: 23,670,255
  Trainable: 23,440,879
  Modules:
    â”œâ”€â”€ Thalamus (Gateway)
    â”œâ”€â”€ Cortex (Processing)
    â”œâ”€â”€ Working Memory (PFC)
    â”œâ”€â”€ Episodic Memory (Hippocampus)
    â”œâ”€â”€ Semantic Memory (Neocortex)
    â”œâ”€â”€ Dopamine Circuit (VTA)
    â”œâ”€â”€ Striatum (Decisions)
    â”œâ”€â”€ dACC (Metacognition)
    â”œâ”€â”€ Attention Controller
    â””â”€â”€ Oscillations
============================================================
  Loaded weights successfully
  Parameters: 23,670,255
[Loading] Tokenizer: /workspace/checkpoints/tokenizer.json
[NokaiTokenizer] Loaded from /workspace/checkpoints/tokenizer.json (vocab_size=5996)
  BPE tokenizer loaded (vocab=5996)

Prompt: She picks up the club
----------------------------------------
Response:  was and He She to. to He day and


[Stats: 18 tokens, 17.9 tok/s, DA: 0.63]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘                                                                              â•‘
â•‘                   ðŸ§  NÅŒKAI EXPERIMENT: BLUE APPLE PROTOCOL V2                 â•‘
â•‘                                                                              â•‘
â•‘            CLAMPED Hebbian Learning - Teacher Forcing for Synapses           â•‘
â•‘                                                                              â•‘
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  INITIALIZATION
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Loading NÅkai brain...
[NokaiTokenizer] Loaded from /workspace/checkpoints/tokenizer.json (vocab_size=5996)
  Detected config: vocab=5996, dim=128, seq=512
============================================================
NÅŒKAI NEUROMORPHIC BRAIN
============================================================
  Total parameters: 23,670,255
  Trainable: 23,440,879
  Modules:
    â”œâ”€â”€ Thalamus (Gateway)
    â”œâ”€â”€ Cortex (Processing)
    â”œâ”€â”€ Working Memory (PFC)
    â”œâ”€â”€ Episodic Memory (Hippocampus)
    â”œâ”€â”€ Semantic Memory (Neocortex)
    â”œâ”€â”€ Dopamine Circuit (VTA)
    â”œâ”€â”€ Striatum (Decisions)
    â”œâ”€â”€ dACC (Metacognition)
    â”œâ”€â”€ Attention Controller
    â””â”€â”€ Oscillations
============================================================
  âœ“ Brain loaded successfully

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  PHASE 1: BASELINE
  Measuring initial state before Hebbian learning
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


  ðŸ”¬ SYNAPSE SNAPSHOT: OUTPUT PROJECTION (Before)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    w[  0]=+1.9204 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  w[  1]=+1.4524 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[  2]=+0.8810 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     w[  3]=-2.0814 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[  4]=+0.6373 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          w[  5]=-1.2248 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[  6]=-0.0496                       w[  7]=-1.5805 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[  8]=-0.7602 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       w[  9]=+1.6278 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[ 10]=-0.3722 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               w[ 11]=-1.4090 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[ 12]=-0.6883 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         w[ 13]=-0.5234 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[ 14]=-0.7434 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        w[ 15]=+0.7331 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    w[ 16]=+1.6131 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  w[ 17]=-0.1658 â–ˆâ–ˆâ–ˆ
    w[ 18]=-0.4992 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             w[ 19]=+0.4164 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

  Question: "What color is an apple?"

  Baseline probabilities for next word:
       red        â†’ 0.0000 (0.00%)
       green      â†’ 0.0000 (0.00%)
       yellow     â†’ 0.0000 (0.00%)
       orange     â†’ 0.0000 (0.00%)
    ðŸ”µ blue       â†’ 0.0000 (0.00%)

  Generating baseline response...
  Response: "him"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  PHASE 2: INCEPTION (TEACHER FORCING)
  Injecting 'Blue' via CLAMPED Hebbian learning - no backprop!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Sentence: "In this world, apples are always BLUE."
  Target Word: "blue"
  Hebbian LR: 0.01
  Dopamine Boost: 0.9
  Repetitions: 3

  â•â•â• REPETITION 1/3 â•â•â•

    ðŸŽ¯ Creating target activation for 'blue'...
    âœ“ Target token 'blue' â†’ ID 2

    âš¡ Applying CLAMPED Hebbian to output_projection...
       (This is the Hiddenâ†’Vocab layer that determines word probabilities)
Traceback (most recent call last):
  File "/workspace/Nokai/scripts/experiment_one_shot.py", line 862, in <module>
    main()
  File "/workspace/Nokai/scripts/experiment_one_shot.py", line 858, in main
    experiment.run_experiment()
  File "/workspace/Nokai/scripts/experiment_one_shot.py", line 688, in run_experiment
    inception_stats = self.apply_clamped_hebbian_update(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Nokai/scripts/experiment_one_shot.py", line 540, in apply_clamped_hebbian_update
    success, change = self.output_hebbian.apply_clamped_update(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Nokai/nokai/learning/hebbian_v2.py", line 679, in apply_clamped_update
    self.total_update_magnitude += delta.abs().mean()
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!